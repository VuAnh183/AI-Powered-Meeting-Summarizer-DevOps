{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import textwrap\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 29 key-value pairs and 291 tensors from ./models/mistral_lite/Mistral-7B-Instruct-v0.3-IQ4_XS.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Mistral-7B-Instruct-v0.3\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 30\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  25:                      quantize.imatrix.file str              = /models/Mistral-7B-Instruct-v0.3-GGUF...\n",
      "llama_model_loader: - kv  26:                   quantize.imatrix.dataset str              = /training_data/calibration_data.txt\n",
      "llama_model_loader: - kv  27:             quantize.imatrix.entries_count i32              = 224\n",
      "llama_model_loader: - kv  28:              quantize.imatrix.chunks_count i32              = 228\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llama_model_loader: - type iq4_xs:  193 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = IQ4_XS - 4.25 bpw\n",
      "print_info: file size   = 3.64 GiB (4.32 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:     48 '[control_46]' is not marked as EOG\n",
      "load: control token:    624 '[control_622]' is not marked as EOG\n",
      "load: control token:    216 '[control_214]' is not marked as EOG\n",
      "load: control token:     40 '[control_38]' is not marked as EOG\n",
      "load: control token:    322 '[control_320]' is not marked as EOG\n",
      "load: control token:      4 '[/INST]' is not marked as EOG\n",
      "load: control token:    366 '[control_364]' is not marked as EOG\n",
      "load: control token:     32 '[control_30]' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: control token:    425 '[control_423]' is not marked as EOG\n",
      "load: control token:     44 '[control_42]' is not marked as EOG\n",
      "load: control token:      7 '[/AVAILABLE_TOOLS]' is not marked as EOG\n",
      "load: control token:    206 '[control_204]' is not marked as EOG\n",
      "load: control token:     22 '[control_20]' is not marked as EOG\n",
      "load: control token:    241 '[control_239]' is not marked as EOG\n",
      "load: control token:    112 '[control_110]' is not marked as EOG\n",
      "load: control token:    398 '[control_396]' is not marked as EOG\n",
      "load: control token:      5 '[TOOL_CALLS]' is not marked as EOG\n",
      "load: control token:    655 '[control_653]' is not marked as EOG\n",
      "load: control token:    725 '[control_723]' is not marked as EOG\n",
      "load: control token:    340 '[control_338]' is not marked as EOG\n",
      "load: control token:    523 '[control_521]' is not marked as EOG\n",
      "load: control token:    242 '[control_240]' is not marked as EOG\n",
      "load: control token:      3 '[INST]' is not marked as EOG\n",
      "load: control token:    364 '[control_362]' is not marked as EOG\n",
      "load: control token:     38 '[control_36]' is not marked as EOG\n",
      "load: control token:    333 '[control_331]' is not marked as EOG\n",
      "load: control token:    530 '[control_528]' is not marked as EOG\n",
      "load: control token:    251 '[control_249]' is not marked as EOG\n",
      "load: control token:      6 '[AVAILABLE_TOOLS]' is not marked as EOG\n",
      "load: control token:     63 '[control_61]' is not marked as EOG\n",
      "load: control token:    346 '[control_344]' is not marked as EOG\n",
      "load: control token:     12 '[control_10]' is not marked as EOG\n",
      "load: control token:    409 '[control_407]' is not marked as EOG\n",
      "load: control token:      8 '[TOOL_RESULTS]' is not marked as EOG\n",
      "load: control token:    694 '[control_692]' is not marked as EOG\n",
      "load: control token:    153 '[control_151]' is not marked as EOG\n",
      "load: control token:    374 '[control_372]' is not marked as EOG\n",
      "load: control token:      9 '[/TOOL_RESULTS]' is not marked as EOG\n",
      "load: control token:     59 '[control_57]' is not marked as EOG\n",
      "load: control token:    191 '[control_189]' is not marked as EOG\n",
      "load: control token:    284 '[control_282]' is not marked as EOG\n",
      "load: control token:     10 '[control_8]' is not marked as EOG\n",
      "load: control token:     58 '[control_56]' is not marked as EOG\n",
      "load: control token:    190 '[control_188]' is not marked as EOG\n",
      "load: control token:    285 '[control_283]' is not marked as EOG\n",
      "load: control token:     11 '[control_9]' is not marked as EOG\n",
      "load: control token:     62 '[control_60]' is not marked as EOG\n",
      "load: control token:    347 '[control_345]' is not marked as EOG\n",
      "load: control token:     13 '[control_11]' is not marked as EOG\n",
      "load: control token:    348 '[control_346]' is not marked as EOG\n",
      "load: control token:     14 '[control_12]' is not marked as EOG\n",
      "load: control token:    349 '[control_347]' is not marked as EOG\n",
      "load: control token:     15 '[control_13]' is not marked as EOG\n",
      "load: control token:    698 '[control_696]' is not marked as EOG\n",
      "load: control token:    405 '[control_403]' is not marked as EOG\n",
      "load: control token:    157 '[control_155]' is not marked as EOG\n",
      "load: control token:     60 '[control_58]' is not marked as EOG\n",
      "load: control token:    342 '[control_340]' is not marked as EOG\n",
      "load: control token:     16 '[control_14]' is not marked as EOG\n",
      "load: control token:    699 '[control_697]' is not marked as EOG\n",
      "load: control token:    404 '[control_402]' is not marked as EOG\n",
      "load: control token:    156 '[control_154]' is not marked as EOG\n",
      "load: control token:     61 '[control_59]' is not marked as EOG\n",
      "load: control token:    343 '[control_341]' is not marked as EOG\n",
      "load: control token:     17 '[control_15]' is not marked as EOG\n",
      "load: control token:    344 '[control_342]' is not marked as EOG\n",
      "load: control token:     18 '[control_16]' is not marked as EOG\n",
      "load: control token:    345 '[control_343]' is not marked as EOG\n",
      "load: control token:     19 '[control_17]' is not marked as EOG\n",
      "load: control token:    161 '[control_159]' is not marked as EOG\n",
      "load: control token:     56 '[control_54]' is not marked as EOG\n",
      "load: control token:     20 '[control_18]' is not marked as EOG\n",
      "load: control token:     57 '[control_55]' is not marked as EOG\n",
      "load: control token:    160 '[control_158]' is not marked as EOG\n",
      "load: control token:     21 '[control_19]' is not marked as EOG\n",
      "load: control token:    207 '[control_205]' is not marked as EOG\n",
      "load: control token:     23 '[control_21]' is not marked as EOG\n",
      "load: control token:     24 '[control_22]' is not marked as EOG\n",
      "load: control token:    208 '[control_206]' is not marked as EOG\n",
      "load: control token:    209 '[control_207]' is not marked as EOG\n",
      "load: control token:     25 '[control_23]' is not marked as EOG\n",
      "load: control token:    380 '[control_378]' is not marked as EOG\n",
      "load: control token:     26 '[control_24]' is not marked as EOG\n",
      "load: control token:    202 '[control_200]' is not marked as EOG\n",
      "load: control token:    381 '[control_379]' is not marked as EOG\n",
      "load: control token:    203 '[control_201]' is not marked as EOG\n",
      "load: control token:     27 '[control_25]' is not marked as EOG\n",
      "load: control token:     28 '[control_26]' is not marked as EOG\n",
      "load: control token:    204 '[control_202]' is not marked as EOG\n",
      "load: control token:    205 '[control_203]' is not marked as EOG\n",
      "load: control token:     29 '[control_27]' is not marked as EOG\n",
      "load: control token:    376 '[control_374]' is not marked as EOG\n",
      "load: control token:     30 '[control_28]' is not marked as EOG\n",
      "load: control token:    377 '[control_375]' is not marked as EOG\n",
      "load: control token:     31 '[control_29]' is not marked as EOG\n",
      "load: control token:    367 '[control_365]' is not marked as EOG\n",
      "load: control token:     33 '[control_31]' is not marked as EOG\n",
      "load: control token:    368 '[control_366]' is not marked as EOG\n",
      "load: control token:     34 '[control_32]' is not marked as EOG\n",
      "load: control token:    369 '[control_367]' is not marked as EOG\n",
      "load: control token:     35 '[control_33]' is not marked as EOG\n",
      "load: control token:    362 '[control_360]' is not marked as EOG\n",
      "load: control token:    220 '[control_218]' is not marked as EOG\n",
      "load: control token:     36 '[control_34]' is not marked as EOG\n",
      "load: control token:    363 '[control_361]' is not marked as EOG\n",
      "load: control token:    221 '[control_219]' is not marked as EOG\n",
      "load: control token:     37 '[control_35]' is not marked as EOG\n",
      "load: control token:    365 '[control_363]' is not marked as EOG\n",
      "load: control token:     39 '[control_37]' is not marked as EOG\n",
      "load: control token:     41 '[control_39]' is not marked as EOG\n",
      "load: control token:    217 '[control_215]' is not marked as EOG\n",
      "load: control token:     42 '[control_40]' is not marked as EOG\n",
      "load: control token:     43 '[control_41]' is not marked as EOG\n",
      "load: control token:     45 '[control_43]' is not marked as EOG\n",
      "load: control token:    481 '[control_479]' is not marked as EOG\n",
      "load: control token:     46 '[control_44]' is not marked as EOG\n",
      "load: control token:    480 '[control_478]' is not marked as EOG\n",
      "load: control token:     47 '[control_45]' is not marked as EOG\n",
      "load: control token:     49 '[control_47]' is not marked as EOG\n",
      "load: control token:    477 '[control_475]' is not marked as EOG\n",
      "load: control token:     50 '[control_48]' is not marked as EOG\n",
      "load: control token:    476 '[control_474]' is not marked as EOG\n",
      "load: control token:     51 '[control_49]' is not marked as EOG\n",
      "load: control token:     52 '[control_50]' is not marked as EOG\n",
      "load: control token:     53 '[control_51]' is not marked as EOG\n",
      "load: control token:    411 '[control_409]' is not marked as EOG\n",
      "load: control token:     54 '[control_52]' is not marked as EOG\n",
      "load: control token:    410 '[control_408]' is not marked as EOG\n",
      "load: control token:     55 '[control_53]' is not marked as EOG\n",
      "load: control token:     64 '[control_62]' is not marked as EOG\n",
      "load: control token:     65 '[control_63]' is not marked as EOG\n",
      "load: control token:     66 '[control_64]' is not marked as EOG\n",
      "load: control token:     67 '[control_65]' is not marked as EOG\n",
      "load: control token:     68 '[control_66]' is not marked as EOG\n",
      "load: control token:     69 '[control_67]' is not marked as EOG\n",
      "load: control token:     70 '[control_68]' is not marked as EOG\n",
      "load: control token:     71 '[control_69]' is not marked as EOG\n",
      "load: control token:     72 '[control_70]' is not marked as EOG\n",
      "load: control token:     73 '[control_71]' is not marked as EOG\n",
      "load: control token:     74 '[control_72]' is not marked as EOG\n",
      "load: control token:     75 '[control_73]' is not marked as EOG\n",
      "load: control token:     76 '[control_74]' is not marked as EOG\n",
      "load: control token:     77 '[control_75]' is not marked as EOG\n",
      "load: control token:     78 '[control_76]' is not marked as EOG\n",
      "load: control token:     79 '[control_77]' is not marked as EOG\n",
      "load: control token:     80 '[control_78]' is not marked as EOG\n",
      "load: control token:     81 '[control_79]' is not marked as EOG\n",
      "load: control token:     82 '[control_80]' is not marked as EOG\n",
      "load: control token:     83 '[control_81]' is not marked as EOG\n",
      "load: control token:     84 '[control_82]' is not marked as EOG\n",
      "load: control token:     85 '[control_83]' is not marked as EOG\n",
      "load: control token:     86 '[control_84]' is not marked as EOG\n",
      "load: control token:     87 '[control_85]' is not marked as EOG\n",
      "load: control token:     88 '[control_86]' is not marked as EOG\n",
      "load: control token:     89 '[control_87]' is not marked as EOG\n",
      "load: control token:     90 '[control_88]' is not marked as EOG\n",
      "load: control token:     91 '[control_89]' is not marked as EOG\n",
      "load: control token:     92 '[control_90]' is not marked as EOG\n",
      "load: control token:     93 '[control_91]' is not marked as EOG\n",
      "load: control token:     94 '[control_92]' is not marked as EOG\n",
      "load: control token:     95 '[control_93]' is not marked as EOG\n",
      "load: control token:     96 '[control_94]' is not marked as EOG\n",
      "load: control token:     97 '[control_95]' is not marked as EOG\n",
      "load: control token:     98 '[control_96]' is not marked as EOG\n",
      "load: control token:     99 '[control_97]' is not marked as EOG\n",
      "load: control token:    100 '[control_98]' is not marked as EOG\n",
      "load: control token:    101 '[control_99]' is not marked as EOG\n",
      "load: control token:    454 '[control_452]' is not marked as EOG\n",
      "load: control token:    102 '[control_100]' is not marked as EOG\n",
      "load: control token:    455 '[control_453]' is not marked as EOG\n",
      "load: control token:    103 '[control_101]' is not marked as EOG\n",
      "load: control token:    452 '[control_450]' is not marked as EOG\n",
      "load: control token:    104 '[control_102]' is not marked as EOG\n",
      "load: control token:    390 '[control_388]' is not marked as EOG\n",
      "load: control token:    453 '[control_451]' is not marked as EOG\n",
      "load: control token:    105 '[control_103]' is not marked as EOG\n",
      "load: control token:    391 '[control_389]' is not marked as EOG\n",
      "load: control token:    458 '[control_456]' is not marked as EOG\n",
      "load: control token:    106 '[control_104]' is not marked as EOG\n",
      "load: control token:    459 '[control_457]' is not marked as EOG\n",
      "load: control token:    107 '[control_105]' is not marked as EOG\n",
      "load: control token:    456 '[control_454]' is not marked as EOG\n",
      "load: control token:    108 '[control_106]' is not marked as EOG\n",
      "load: control token:    457 '[control_455]' is not marked as EOG\n",
      "load: control token:    109 '[control_107]' is not marked as EOG\n",
      "load: control token:    110 '[control_108]' is not marked as EOG\n",
      "load: control token:    384 '[control_382]' is not marked as EOG\n",
      "load: control token:    111 '[control_109]' is not marked as EOG\n",
      "load: control token:    385 '[control_383]' is not marked as EOG\n",
      "load: control token:    240 '[control_238]' is not marked as EOG\n",
      "load: control token:    113 '[control_111]' is not marked as EOG\n",
      "load: control token:    399 '[control_397]' is not marked as EOG\n",
      "load: control token:    114 '[control_112]' is not marked as EOG\n",
      "load: control token:    396 '[control_394]' is not marked as EOG\n",
      "load: control token:    115 '[control_113]' is not marked as EOG\n",
      "load: control token:    397 '[control_395]' is not marked as EOG\n",
      "load: control token:    116 '[control_114]' is not marked as EOG\n",
      "load: control token:    394 '[control_392]' is not marked as EOG\n",
      "load: control token:    395 '[control_393]' is not marked as EOG\n",
      "load: control token:    117 '[control_115]' is not marked as EOG\n",
      "load: control token:    450 '[control_448]' is not marked as EOG\n",
      "load: control token:    657 '[control_655]' is not marked as EOG\n",
      "load: control token:    118 '[control_116]' is not marked as EOG\n",
      "load: control token:    392 '[control_390]' is not marked as EOG\n",
      "load: control token:    656 '[control_654]' is not marked as EOG\n",
      "load: control token:    451 '[control_449]' is not marked as EOG\n",
      "load: control token:    119 '[control_117]' is not marked as EOG\n",
      "load: control token:    393 '[control_391]' is not marked as EOG\n",
      "load: control token:    448 '[control_446]' is not marked as EOG\n",
      "load: control token:    120 '[control_118]' is not marked as EOG\n",
      "load: control token:    233 '[control_231]' is not marked as EOG\n",
      "load: control token:    449 '[control_447]' is not marked as EOG\n",
      "load: control token:    121 '[control_119]' is not marked as EOG\n",
      "load: control token:    232 '[control_230]' is not marked as EOG\n",
      "load: control token:    122 '[control_120]' is not marked as EOG\n",
      "load: control token:    123 '[control_121]' is not marked as EOG\n",
      "load: control token:    124 '[control_122]' is not marked as EOG\n",
      "load: control token:    125 '[control_123]' is not marked as EOG\n",
      "load: control token:    126 '[control_124]' is not marked as EOG\n",
      "load: control token:    127 '[control_125]' is not marked as EOG\n",
      "load: control token:    510 '[control_508]' is not marked as EOG\n",
      "load: control token:    231 '[control_229]' is not marked as EOG\n",
      "load: control token:    128 '[control_126]' is not marked as EOG\n",
      "load: control token:    129 '[control_127]' is not marked as EOG\n",
      "load: control token:    230 '[control_228]' is not marked as EOG\n",
      "load: control token:    511 '[control_509]' is not marked as EOG\n",
      "load: control token:    130 '[control_128]' is not marked as EOG\n",
      "load: control token:    229 '[control_227]' is not marked as EOG\n",
      "load: control token:    508 '[control_506]' is not marked as EOG\n",
      "load: control token:    131 '[control_129]' is not marked as EOG\n",
      "load: control token:    228 '[control_226]' is not marked as EOG\n",
      "load: control token:    509 '[control_507]' is not marked as EOG\n",
      "load: control token:    132 '[control_130]' is not marked as EOG\n",
      "load: control token:    261 '[control_259]' is not marked as EOG\n",
      "load: control token:    133 '[control_131]' is not marked as EOG\n",
      "load: control token:    260 '[control_258]' is not marked as EOG\n",
      "load: control token:    520 '[control_518]' is not marked as EOG\n",
      "load: control token:    134 '[control_132]' is not marked as EOG\n",
      "load: control token:    135 '[control_133]' is not marked as EOG\n",
      "load: control token:    521 '[control_519]' is not marked as EOG\n",
      "load: control token:    136 '[control_134]' is not marked as EOG\n",
      "load: control token:    137 '[control_135]' is not marked as EOG\n",
      "load: control token:    138 '[control_136]' is not marked as EOG\n",
      "load: control token:    139 '[control_137]' is not marked as EOG\n",
      "load: control token:    514 '[control_512]' is not marked as EOG\n",
      "load: control token:    140 '[control_138]' is not marked as EOG\n",
      "load: control token:    253 '[control_251]' is not marked as EOG\n",
      "load: control token:    515 '[control_513]' is not marked as EOG\n",
      "load: control token:    141 '[control_139]' is not marked as EOG\n",
      "load: control token:    252 '[control_250]' is not marked as EOG\n",
      "load: control token:    414 '[control_412]' is not marked as EOG\n",
      "load: control token:    142 '[control_140]' is not marked as EOG\n",
      "load: control token:    415 '[control_413]' is not marked as EOG\n",
      "load: control token:    143 '[control_141]' is not marked as EOG\n",
      "load: control token:    412 '[control_410]' is not marked as EOG\n",
      "load: control token:    144 '[control_142]' is not marked as EOG\n",
      "load: control token:    413 '[control_411]' is not marked as EOG\n",
      "load: control token:    145 '[control_143]' is not marked as EOG\n",
      "load: control token:    418 '[control_416]' is not marked as EOG\n",
      "load: control token:    146 '[control_144]' is not marked as EOG\n",
      "load: control token:    419 '[control_417]' is not marked as EOG\n",
      "load: control token:    147 '[control_145]' is not marked as EOG\n",
      "load: control token:    416 '[control_414]' is not marked as EOG\n",
      "load: control token:    148 '[control_146]' is not marked as EOG\n",
      "load: control token:    417 '[control_415]' is not marked as EOG\n",
      "load: control token:    149 '[control_147]' is not marked as EOG\n",
      "load: control token:    150 '[control_148]' is not marked as EOG\n",
      "load: control token:    151 '[control_149]' is not marked as EOG\n",
      "load: control token:    695 '[control_693]' is not marked as EOG\n",
      "load: control token:    408 '[control_406]' is not marked as EOG\n",
      "load: control token:    152 '[control_150]' is not marked as EOG\n",
      "load: control token:    406 '[control_404]' is not marked as EOG\n",
      "load: control token:    693 '[control_691]' is not marked as EOG\n",
      "load: control token:    154 '[control_152]' is not marked as EOG\n",
      "load: control token:    692 '[control_690]' is not marked as EOG\n",
      "load: control token:    407 '[control_405]' is not marked as EOG\n",
      "load: control token:    155 '[control_153]' is not marked as EOG\n",
      "load: control token:    697 '[control_695]' is not marked as EOG\n",
      "load: control token:    402 '[control_400]' is not marked as EOG\n",
      "load: control token:    158 '[control_156]' is not marked as EOG\n",
      "load: control token:    696 '[control_694]' is not marked as EOG\n",
      "load: control token:    403 '[control_401]' is not marked as EOG\n",
      "load: control token:    159 '[control_157]' is not marked as EOG\n",
      "load: control token:    162 '[control_160]' is not marked as EOG\n",
      "load: control token:    163 '[control_161]' is not marked as EOG\n",
      "load: control token:    164 '[control_162]' is not marked as EOG\n",
      "load: control token:    165 '[control_163]' is not marked as EOG\n",
      "load: control token:    166 '[control_164]' is not marked as EOG\n",
      "load: control token:    167 '[control_165]' is not marked as EOG\n",
      "load: control token:    761 '[control_759]' is not marked as EOG\n",
      "load: control token:    168 '[control_166]' is not marked as EOG\n",
      "load: control token:    760 '[control_758]' is not marked as EOG\n",
      "load: control token:    169 '[control_167]' is not marked as EOG\n",
      "load: control token:    759 '[control_757]' is not marked as EOG\n",
      "load: control token:    170 '[control_168]' is not marked as EOG\n",
      "load: control token:    758 '[control_756]' is not marked as EOG\n",
      "load: control token:    171 '[control_169]' is not marked as EOG\n",
      "load: control token:    464 '[control_462]' is not marked as EOG\n",
      "load: control token:    172 '[control_170]' is not marked as EOG\n",
      "load: control token:    465 '[control_463]' is not marked as EOG\n",
      "load: control token:    173 '[control_171]' is not marked as EOG\n",
      "load: control token:    462 '[control_460]' is not marked as EOG\n",
      "load: control token:    174 '[control_172]' is not marked as EOG\n",
      "load: control token:    463 '[control_461]' is not marked as EOG\n",
      "load: control token:    175 '[control_173]' is not marked as EOG\n",
      "load: control token:    468 '[control_466]' is not marked as EOG\n",
      "load: control token:    176 '[control_174]' is not marked as EOG\n",
      "load: control token:    469 '[control_467]' is not marked as EOG\n",
      "load: control token:    177 '[control_175]' is not marked as EOG\n",
      "load: control token:    466 '[control_464]' is not marked as EOG\n",
      "load: control token:    178 '[control_176]' is not marked as EOG\n",
      "load: control token:    467 '[control_465]' is not marked as EOG\n",
      "load: control token:    179 '[control_177]' is not marked as EOG\n",
      "load: control token:    180 '[control_178]' is not marked as EOG\n",
      "load: control token:    181 '[control_179]' is not marked as EOG\n",
      "load: control token:    182 '[control_180]' is not marked as EOG\n",
      "load: control token:    183 '[control_181]' is not marked as EOG\n",
      "load: control token:    184 '[control_182]' is not marked as EOG\n",
      "load: control token:    291 '[control_289]' is not marked as EOG\n",
      "load: control token:    185 '[control_183]' is not marked as EOG\n",
      "load: control token:    290 '[control_288]' is not marked as EOG\n",
      "load: control token:    186 '[control_184]' is not marked as EOG\n",
      "load: control token:    187 '[control_185]' is not marked as EOG\n",
      "load: control token:    188 '[control_186]' is not marked as EOG\n",
      "load: control token:    189 '[control_187]' is not marked as EOG\n",
      "load: control token:    192 '[control_190]' is not marked as EOG\n",
      "load: control token:    318 '[control_316]' is not marked as EOG\n",
      "load: control token:    193 '[control_191]' is not marked as EOG\n",
      "load: control token:    319 '[control_317]' is not marked as EOG\n",
      "load: control token:    194 '[control_192]' is not marked as EOG\n",
      "load: control token:    316 '[control_314]' is not marked as EOG\n",
      "load: control token:    195 '[control_193]' is not marked as EOG\n",
      "load: control token:    317 '[control_315]' is not marked as EOG\n",
      "load: control token:    196 '[control_194]' is not marked as EOG\n",
      "load: control token:    314 '[control_312]' is not marked as EOG\n",
      "load: control token:    315 '[control_313]' is not marked as EOG\n",
      "load: control token:    197 '[control_195]' is not marked as EOG\n",
      "load: control token:    198 '[control_196]' is not marked as EOG\n",
      "load: control token:    312 '[control_310]' is not marked as EOG\n",
      "load: control token:    551 '[control_549]' is not marked as EOG\n",
      "load: control token:    270 '[control_268]' is not marked as EOG\n",
      "load: control token:    199 '[control_197]' is not marked as EOG\n",
      "load: control token:    313 '[control_311]' is not marked as EOG\n",
      "load: control token:    550 '[control_548]' is not marked as EOG\n",
      "load: control token:    271 '[control_269]' is not marked as EOG\n",
      "load: control token:    268 '[control_266]' is not marked as EOG\n",
      "load: control token:    549 '[control_547]' is not marked as EOG\n",
      "load: control token:    200 '[control_198]' is not marked as EOG\n",
      "load: control token:    548 '[control_546]' is not marked as EOG\n",
      "load: control token:    269 '[control_267]' is not marked as EOG\n",
      "load: control token:    201 '[control_199]' is not marked as EOG\n",
      "load: control token:    372 '[control_370]' is not marked as EOG\n",
      "load: control token:    210 '[control_208]' is not marked as EOG\n",
      "load: control token:    373 '[control_371]' is not marked as EOG\n",
      "load: control token:    211 '[control_209]' is not marked as EOG\n",
      "load: control token:    370 '[control_368]' is not marked as EOG\n",
      "load: control token:    212 '[control_210]' is not marked as EOG\n",
      "load: control token:    371 '[control_369]' is not marked as EOG\n",
      "load: control token:    213 '[control_211]' is not marked as EOG\n",
      "load: control token:    214 '[control_212]' is not marked as EOG\n",
      "load: control token:    215 '[control_213]' is not marked as EOG\n",
      "load: control token:    218 '[control_216]' is not marked as EOG\n",
      "load: control token:    219 '[control_217]' is not marked as EOG\n",
      "load: control token:    222 '[control_220]' is not marked as EOG\n",
      "load: control token:    503 '[control_501]' is not marked as EOG\n",
      "load: control token:    502 '[control_500]' is not marked as EOG\n",
      "load: control token:    223 '[control_221]' is not marked as EOG\n",
      "load: control token:    224 '[control_222]' is not marked as EOG\n",
      "load: control token:    505 '[control_503]' is not marked as EOG\n",
      "load: control token:    504 '[control_502]' is not marked as EOG\n",
      "load: control token:    225 '[control_223]' is not marked as EOG\n",
      "load: control token:    226 '[control_224]' is not marked as EOG\n",
      "load: control token:    507 '[control_505]' is not marked as EOG\n",
      "load: control token:    227 '[control_225]' is not marked as EOG\n",
      "load: control token:    506 '[control_504]' is not marked as EOG\n",
      "load: control token:    660 '[control_658]' is not marked as EOG\n",
      "load: control token:    447 '[control_445]' is not marked as EOG\n",
      "load: control token:    234 '[control_232]' is not marked as EOG\n",
      "load: control token:    661 '[control_659]' is not marked as EOG\n",
      "load: control token:    446 '[control_444]' is not marked as EOG\n",
      "load: control token:    235 '[control_233]' is not marked as EOG\n",
      "load: control token:    445 '[control_443]' is not marked as EOG\n",
      "load: control token:    236 '[control_234]' is not marked as EOG\n",
      "load: control token:    444 '[control_442]' is not marked as EOG\n",
      "load: control token:    237 '[control_235]' is not marked as EOG\n",
      "load: control token:    443 '[control_441]' is not marked as EOG\n",
      "load: control token:    238 '[control_236]' is not marked as EOG\n",
      "load: control token:    401 '[control_399]' is not marked as EOG\n",
      "load: control token:    442 '[control_440]' is not marked as EOG\n",
      "load: control token:    239 '[control_237]' is not marked as EOG\n",
      "load: control token:    400 '[control_398]' is not marked as EOG\n",
      "load: control token:    341 '[control_339]' is not marked as EOG\n",
      "load: control token:    724 '[control_722]' is not marked as EOG\n",
      "load: control token:    522 '[control_520]' is not marked as EOG\n",
      "load: control token:    243 '[control_241]' is not marked as EOG\n",
      "load: control token:    525 '[control_523]' is not marked as EOG\n",
      "load: control token:    244 '[control_242]' is not marked as EOG\n",
      "load: control token:    245 '[control_243]' is not marked as EOG\n",
      "load: control token:    524 '[control_522]' is not marked as EOG\n",
      "load: control token:    246 '[control_244]' is not marked as EOG\n",
      "load: control token:    527 '[control_525]' is not marked as EOG\n",
      "load: control token:    526 '[control_524]' is not marked as EOG\n",
      "load: control token:    247 '[control_245]' is not marked as EOG\n",
      "load: control token:    529 '[control_527]' is not marked as EOG\n",
      "load: control token:    248 '[control_246]' is not marked as EOG\n",
      "load: control token:    249 '[control_247]' is not marked as EOG\n",
      "load: control token:    528 '[control_526]' is not marked as EOG\n",
      "load: control token:    332 '[control_330]' is not marked as EOG\n",
      "load: control token:    531 '[control_529]' is not marked as EOG\n",
      "load: control token:    250 '[control_248]' is not marked as EOG\n",
      "load: control token:    254 '[control_252]' is not marked as EOG\n",
      "load: control token:    513 '[control_511]' is not marked as EOG\n",
      "load: control token:    255 '[control_253]' is not marked as EOG\n",
      "load: control token:    512 '[control_510]' is not marked as EOG\n",
      "load: control token:    519 '[control_517]' is not marked as EOG\n",
      "load: control token:    256 '[control_254]' is not marked as EOG\n",
      "load: control token:    518 '[control_516]' is not marked as EOG\n",
      "load: control token:    257 '[control_255]' is not marked as EOG\n",
      "load: control token:    517 '[control_515]' is not marked as EOG\n",
      "load: control token:    258 '[control_256]' is not marked as EOG\n",
      "load: control token:    516 '[control_514]' is not marked as EOG\n",
      "load: control token:    259 '[control_257]' is not marked as EOG\n",
      "load: control token:    320 '[control_318]' is not marked as EOG\n",
      "load: control token:    543 '[control_541]' is not marked as EOG\n",
      "load: control token:    262 '[control_260]' is not marked as EOG\n",
      "load: control token:    321 '[control_319]' is not marked as EOG\n",
      "load: control token:    542 '[control_540]' is not marked as EOG\n",
      "load: control token:    263 '[control_261]' is not marked as EOG\n",
      "load: control token:    264 '[control_262]' is not marked as EOG\n",
      "load: control token:    545 '[control_543]' is not marked as EOG\n",
      "load: control token:    544 '[control_542]' is not marked as EOG\n",
      "load: control token:    265 '[control_263]' is not marked as EOG\n",
      "load: control token:    547 '[control_545]' is not marked as EOG\n",
      "load: control token:    266 '[control_264]' is not marked as EOG\n",
      "load: control token:    267 '[control_265]' is not marked as EOG\n",
      "load: control token:    546 '[control_544]' is not marked as EOG\n",
      "load: control token:    310 '[control_308]' is not marked as EOG\n",
      "load: control token:    733 '[control_731]' is not marked as EOG\n",
      "load: control token:    539 '[control_537]' is not marked as EOG\n",
      "load: control token:    272 '[control_270]' is not marked as EOG\n",
      "load: control token:    311 '[control_309]' is not marked as EOG\n",
      "load: control token:    732 '[control_730]' is not marked as EOG\n",
      "load: control token:    273 '[control_271]' is not marked as EOG\n",
      "load: control token:    538 '[control_536]' is not marked as EOG\n",
      "load: control token:    537 '[control_535]' is not marked as EOG\n",
      "load: control token:    274 '[control_272]' is not marked as EOG\n",
      "load: control token:    536 '[control_534]' is not marked as EOG\n",
      "load: control token:    275 '[control_273]' is not marked as EOG\n",
      "load: control token:    535 '[control_533]' is not marked as EOG\n",
      "load: control token:    276 '[control_274]' is not marked as EOG\n",
      "load: control token:    534 '[control_532]' is not marked as EOG\n",
      "load: control token:    277 '[control_275]' is not marked as EOG\n",
      "load: control token:    533 '[control_531]' is not marked as EOG\n",
      "load: control token:    278 '[control_276]' is not marked as EOG\n",
      "load: control token:    532 '[control_530]' is not marked as EOG\n",
      "load: control token:    279 '[control_277]' is not marked as EOG\n",
      "load: control token:    741 '[control_739]' is not marked as EOG\n",
      "load: control token:    302 '[control_300]' is not marked as EOG\n",
      "load: control token:    280 '[control_278]' is not marked as EOG\n",
      "load: control token:    303 '[control_301]' is not marked as EOG\n",
      "load: control token:    740 '[control_738]' is not marked as EOG\n",
      "load: control token:    281 '[control_279]' is not marked as EOG\n",
      "load: control token:    282 '[control_280]' is not marked as EOG\n",
      "load: control token:    283 '[control_281]' is not marked as EOG\n",
      "load: control token:    286 '[control_284]' is not marked as EOG\n",
      "load: control token:    287 '[control_285]' is not marked as EOG\n",
      "load: control token:    288 '[control_286]' is not marked as EOG\n",
      "load: control token:    289 '[control_287]' is not marked as EOG\n",
      "load: control token:    292 '[control_290]' is not marked as EOG\n",
      "load: control token:    293 '[control_291]' is not marked as EOG\n",
      "load: control token:    294 '[control_292]' is not marked as EOG\n",
      "load: control token:    295 '[control_293]' is not marked as EOG\n",
      "load: control token:    296 '[control_294]' is not marked as EOG\n",
      "load: control token:    297 '[control_295]' is not marked as EOG\n",
      "load: control token:    298 '[control_296]' is not marked as EOG\n",
      "load: control token:    299 '[control_297]' is not marked as EOG\n",
      "load: control token:    300 '[control_298]' is not marked as EOG\n",
      "load: control token:    301 '[control_299]' is not marked as EOG\n",
      "load: control token:    304 '[control_302]' is not marked as EOG\n",
      "load: control token:    305 '[control_303]' is not marked as EOG\n",
      "load: control token:    306 '[control_304]' is not marked as EOG\n",
      "load: control token:    307 '[control_305]' is not marked as EOG\n",
      "load: control token:    308 '[control_306]' is not marked as EOG\n",
      "load: control token:    309 '[control_307]' is not marked as EOG\n",
      "load: control token:    323 '[control_321]' is not marked as EOG\n",
      "load: control token:    324 '[control_322]' is not marked as EOG\n",
      "load: control token:    325 '[control_323]' is not marked as EOG\n",
      "load: control token:    326 '[control_324]' is not marked as EOG\n",
      "load: control token:    327 '[control_325]' is not marked as EOG\n",
      "load: control token:    328 '[control_326]' is not marked as EOG\n",
      "load: control token:    329 '[control_327]' is not marked as EOG\n",
      "load: control token:    330 '[control_328]' is not marked as EOG\n",
      "load: control token:    331 '[control_329]' is not marked as EOG\n",
      "load: control token:    334 '[control_332]' is not marked as EOG\n",
      "load: control token:    731 '[control_729]' is not marked as EOG\n",
      "load: control token:    730 '[control_728]' is not marked as EOG\n",
      "load: control token:    335 '[control_333]' is not marked as EOG\n",
      "load: control token:    336 '[control_334]' is not marked as EOG\n",
      "load: control token:    337 '[control_335]' is not marked as EOG\n",
      "load: control token:    338 '[control_336]' is not marked as EOG\n",
      "load: control token:    339 '[control_337]' is not marked as EOG\n",
      "load: control token:    350 '[control_348]' is not marked as EOG\n",
      "load: control token:    351 '[control_349]' is not marked as EOG\n",
      "load: control token:    352 '[control_350]' is not marked as EOG\n",
      "load: control token:    353 '[control_351]' is not marked as EOG\n",
      "load: control token:    354 '[control_352]' is not marked as EOG\n",
      "load: control token:    355 '[control_353]' is not marked as EOG\n",
      "load: control token:    356 '[control_354]' is not marked as EOG\n",
      "load: control token:    357 '[control_355]' is not marked as EOG\n",
      "load: control token:    358 '[control_356]' is not marked as EOG\n",
      "load: control token:    359 '[control_357]' is not marked as EOG\n",
      "load: control token:    360 '[control_358]' is not marked as EOG\n",
      "load: control token:    361 '[control_359]' is not marked as EOG\n",
      "load: control token:    375 '[control_373]' is not marked as EOG\n",
      "load: control token:    378 '[control_376]' is not marked as EOG\n",
      "load: control token:    379 '[control_377]' is not marked as EOG\n",
      "load: control token:    460 '[control_458]' is not marked as EOG\n",
      "load: control token:    382 '[control_380]' is not marked as EOG\n",
      "load: control token:    461 '[control_459]' is not marked as EOG\n",
      "load: control token:    383 '[control_381]' is not marked as EOG\n",
      "load: control token:    386 '[control_384]' is not marked as EOG\n",
      "load: control token:    387 '[control_385]' is not marked as EOG\n",
      "load: control token:    388 '[control_386]' is not marked as EOG\n",
      "load: control token:    389 '[control_387]' is not marked as EOG\n",
      "load: control token:    420 '[control_418]' is not marked as EOG\n",
      "load: control token:    421 '[control_419]' is not marked as EOG\n",
      "load: control token:    422 '[control_420]' is not marked as EOG\n",
      "load: control token:    423 '[control_421]' is not marked as EOG\n",
      "load: control token:    424 '[control_422]' is not marked as EOG\n",
      "load: control token:    426 '[control_424]' is not marked as EOG\n",
      "load: control token:    427 '[control_425]' is not marked as EOG\n",
      "load: control token:    428 '[control_426]' is not marked as EOG\n",
      "load: control token:    429 '[control_427]' is not marked as EOG\n",
      "load: control token:    430 '[control_428]' is not marked as EOG\n",
      "load: control token:    431 '[control_429]' is not marked as EOG\n",
      "load: control token:    432 '[control_430]' is not marked as EOG\n",
      "load: control token:    433 '[control_431]' is not marked as EOG\n",
      "load: control token:    434 '[control_432]' is not marked as EOG\n",
      "load: control token:    435 '[control_433]' is not marked as EOG\n",
      "load: control token:    436 '[control_434]' is not marked as EOG\n",
      "load: control token:    437 '[control_435]' is not marked as EOG\n",
      "load: control token:    438 '[control_436]' is not marked as EOG\n",
      "load: control token:    439 '[control_437]' is not marked as EOG\n",
      "load: control token:    440 '[control_438]' is not marked as EOG\n",
      "load: control token:    441 '[control_439]' is not marked as EOG\n",
      "load: control token:    470 '[control_468]' is not marked as EOG\n",
      "load: control token:    471 '[control_469]' is not marked as EOG\n",
      "load: control token:    472 '[control_470]' is not marked as EOG\n",
      "load: control token:    473 '[control_471]' is not marked as EOG\n",
      "load: control token:    474 '[control_472]' is not marked as EOG\n",
      "load: control token:    475 '[control_473]' is not marked as EOG\n",
      "load: control token:    478 '[control_476]' is not marked as EOG\n",
      "load: control token:    479 '[control_477]' is not marked as EOG\n",
      "load: control token:    617 '[control_615]' is not marked as EOG\n",
      "load: control token:    482 '[control_480]' is not marked as EOG\n",
      "load: control token:    616 '[control_614]' is not marked as EOG\n",
      "load: control token:    483 '[control_481]' is not marked as EOG\n",
      "load: control token:    619 '[control_617]' is not marked as EOG\n",
      "load: control token:    484 '[control_482]' is not marked as EOG\n",
      "load: control token:    618 '[control_616]' is not marked as EOG\n",
      "load: control token:    485 '[control_483]' is not marked as EOG\n",
      "load: control token:    613 '[control_611]' is not marked as EOG\n",
      "load: control token:    486 '[control_484]' is not marked as EOG\n",
      "load: control token:    612 '[control_610]' is not marked as EOG\n",
      "load: control token:    487 '[control_485]' is not marked as EOG\n",
      "load: control token:    615 '[control_613]' is not marked as EOG\n",
      "load: control token:    488 '[control_486]' is not marked as EOG\n",
      "load: control token:    489 '[control_487]' is not marked as EOG\n",
      "load: control token:    614 '[control_612]' is not marked as EOG\n",
      "load: control token:    490 '[control_488]' is not marked as EOG\n",
      "load: control token:    491 '[control_489]' is not marked as EOG\n",
      "load: control token:    665 '[control_663]' is not marked as EOG\n",
      "load: control token:    492 '[control_490]' is not marked as EOG\n",
      "load: control token:    664 '[control_662]' is not marked as EOG\n",
      "load: control token:    493 '[control_491]' is not marked as EOG\n",
      "load: control token:    663 '[control_661]' is not marked as EOG\n",
      "load: control token:    494 '[control_492]' is not marked as EOG\n",
      "load: control token:    662 '[control_660]' is not marked as EOG\n",
      "load: control token:    495 '[control_493]' is not marked as EOG\n",
      "load: control token:    669 '[control_667]' is not marked as EOG\n",
      "load: control token:    496 '[control_494]' is not marked as EOG\n",
      "load: control token:    668 '[control_666]' is not marked as EOG\n",
      "load: control token:    497 '[control_495]' is not marked as EOG\n",
      "load: control token:    667 '[control_665]' is not marked as EOG\n",
      "load: control token:    498 '[control_496]' is not marked as EOG\n",
      "load: control token:    666 '[control_664]' is not marked as EOG\n",
      "load: control token:    499 '[control_497]' is not marked as EOG\n",
      "load: control token:    500 '[control_498]' is not marked as EOG\n",
      "load: control token:    501 '[control_499]' is not marked as EOG\n",
      "load: control token:    540 '[control_538]' is not marked as EOG\n",
      "load: control token:    541 '[control_539]' is not marked as EOG\n",
      "load: control token:    552 '[control_550]' is not marked as EOG\n",
      "load: control token:    553 '[control_551]' is not marked as EOG\n",
      "load: control token:    554 '[control_552]' is not marked as EOG\n",
      "load: control token:    555 '[control_553]' is not marked as EOG\n",
      "load: control token:    556 '[control_554]' is not marked as EOG\n",
      "load: control token:    557 '[control_555]' is not marked as EOG\n",
      "load: control token:    558 '[control_556]' is not marked as EOG\n",
      "load: control token:    559 '[control_557]' is not marked as EOG\n",
      "load: control token:    560 '[control_558]' is not marked as EOG\n",
      "load: control token:    561 '[control_559]' is not marked as EOG\n",
      "load: control token:    562 '[control_560]' is not marked as EOG\n",
      "load: control token:    563 '[control_561]' is not marked as EOG\n",
      "load: control token:    564 '[control_562]' is not marked as EOG\n",
      "load: control token:    565 '[control_563]' is not marked as EOG\n",
      "load: control token:    566 '[control_564]' is not marked as EOG\n",
      "load: control token:    567 '[control_565]' is not marked as EOG\n",
      "load: control token:    568 '[control_566]' is not marked as EOG\n",
      "load: control token:    569 '[control_567]' is not marked as EOG\n",
      "load: control token:    570 '[control_568]' is not marked as EOG\n",
      "load: control token:    571 '[control_569]' is not marked as EOG\n",
      "load: control token:    572 '[control_570]' is not marked as EOG\n",
      "load: control token:    573 '[control_571]' is not marked as EOG\n",
      "load: control token:    574 '[control_572]' is not marked as EOG\n",
      "load: control token:    575 '[control_573]' is not marked as EOG\n",
      "load: control token:    576 '[control_574]' is not marked as EOG\n",
      "load: control token:    577 '[control_575]' is not marked as EOG\n",
      "load: control token:    578 '[control_576]' is not marked as EOG\n",
      "load: control token:    579 '[control_577]' is not marked as EOG\n",
      "load: control token:    580 '[control_578]' is not marked as EOG\n",
      "load: control token:    581 '[control_579]' is not marked as EOG\n",
      "load: control token:    582 '[control_580]' is not marked as EOG\n",
      "load: control token:    583 '[control_581]' is not marked as EOG\n",
      "load: control token:    584 '[control_582]' is not marked as EOG\n",
      "load: control token:    585 '[control_583]' is not marked as EOG\n",
      "load: control token:    586 '[control_584]' is not marked as EOG\n",
      "load: control token:    587 '[control_585]' is not marked as EOG\n",
      "load: control token:    588 '[control_586]' is not marked as EOG\n",
      "load: control token:    589 '[control_587]' is not marked as EOG\n",
      "load: control token:    590 '[control_588]' is not marked as EOG\n",
      "load: control token:    591 '[control_589]' is not marked as EOG\n",
      "load: control token:    592 '[control_590]' is not marked as EOG\n",
      "load: control token:    593 '[control_591]' is not marked as EOG\n",
      "load: control token:    594 '[control_592]' is not marked as EOG\n",
      "load: control token:    595 '[control_593]' is not marked as EOG\n",
      "load: control token:    596 '[control_594]' is not marked as EOG\n",
      "load: control token:    690 '[control_688]' is not marked as EOG\n",
      "load: control token:    597 '[control_595]' is not marked as EOG\n",
      "load: control token:    691 '[control_689]' is not marked as EOG\n",
      "load: control token:    598 '[control_596]' is not marked as EOG\n",
      "load: control token:    599 '[control_597]' is not marked as EOG\n",
      "load: control token:    600 '[control_598]' is not marked as EOG\n",
      "load: control token:    686 '[control_684]' is not marked as EOG\n",
      "load: control token:    601 '[control_599]' is not marked as EOG\n",
      "load: control token:    687 '[control_685]' is not marked as EOG\n",
      "load: control token:    711 '[control_709]' is not marked as EOG\n",
      "load: control token:    602 '[control_600]' is not marked as EOG\n",
      "load: control token:    710 '[control_708]' is not marked as EOG\n",
      "load: control token:    603 '[control_601]' is not marked as EOG\n",
      "load: control token:    604 '[control_602]' is not marked as EOG\n",
      "load: control token:    605 '[control_603]' is not marked as EOG\n",
      "load: control token:    606 '[control_604]' is not marked as EOG\n",
      "load: control token:    607 '[control_605]' is not marked as EOG\n",
      "load: control token:    608 '[control_606]' is not marked as EOG\n",
      "load: control token:    609 '[control_607]' is not marked as EOG\n",
      "load: control token:    703 '[control_701]' is not marked as EOG\n",
      "load: control token:    610 '[control_608]' is not marked as EOG\n",
      "load: control token:    702 '[control_700]' is not marked as EOG\n",
      "load: control token:    611 '[control_609]' is not marked as EOG\n",
      "load: control token:    620 '[control_618]' is not marked as EOG\n",
      "load: control token:    621 '[control_619]' is not marked as EOG\n",
      "load: control token:    622 '[control_620]' is not marked as EOG\n",
      "load: control token:    770 '[control_768]' is not marked as EOG\n",
      "load: control token:    623 '[control_621]' is not marked as EOG\n",
      "load: control token:    625 '[control_623]' is not marked as EOG\n",
      "load: control token:    626 '[control_624]' is not marked as EOG\n",
      "load: control token:    627 '[control_625]' is not marked as EOG\n",
      "load: control token:    628 '[control_626]' is not marked as EOG\n",
      "load: control token:    629 '[control_627]' is not marked as EOG\n",
      "load: control token:    763 '[control_761]' is not marked as EOG\n",
      "load: control token:    630 '[control_628]' is not marked as EOG\n",
      "load: control token:    762 '[control_760]' is not marked as EOG\n",
      "load: control token:    631 '[control_629]' is not marked as EOG\n",
      "load: control token:    632 '[control_630]' is not marked as EOG\n",
      "load: control token:    633 '[control_631]' is not marked as EOG\n",
      "load: control token:    634 '[control_632]' is not marked as EOG\n",
      "load: control token:    635 '[control_633]' is not marked as EOG\n",
      "load: control token:    636 '[control_634]' is not marked as EOG\n",
      "load: control token:    637 '[control_635]' is not marked as EOG\n",
      "load: control token:    638 '[control_636]' is not marked as EOG\n",
      "load: control token:    721 '[control_719]' is not marked as EOG\n",
      "load: control token:    639 '[control_637]' is not marked as EOG\n",
      "load: control token:    720 '[control_718]' is not marked as EOG\n",
      "load: control token:    719 '[control_717]' is not marked as EOG\n",
      "load: control token:    640 '[control_638]' is not marked as EOG\n",
      "load: control token:    718 '[control_716]' is not marked as EOG\n",
      "load: control token:    641 '[control_639]' is not marked as EOG\n",
      "load: control token:    642 '[control_640]' is not marked as EOG\n",
      "load: control token:    643 '[control_641]' is not marked as EOG\n",
      "load: control token:    644 '[control_642]' is not marked as EOG\n",
      "load: control token:    645 '[control_643]' is not marked as EOG\n",
      "load: control token:    646 '[control_644]' is not marked as EOG\n",
      "load: control token:    647 '[control_645]' is not marked as EOG\n",
      "load: control token:    648 '[control_646]' is not marked as EOG\n",
      "load: control token:    649 '[control_647]' is not marked as EOG\n",
      "load: control token:    650 '[control_648]' is not marked as EOG\n",
      "load: control token:    651 '[control_649]' is not marked as EOG\n",
      "load: control token:    652 '[control_650]' is not marked as EOG\n",
      "load: control token:    653 '[control_651]' is not marked as EOG\n",
      "load: control token:    654 '[control_652]' is not marked as EOG\n",
      "load: control token:    658 '[control_656]' is not marked as EOG\n",
      "load: control token:    659 '[control_657]' is not marked as EOG\n",
      "load: control token:    670 '[control_668]' is not marked as EOG\n",
      "load: control token:    671 '[control_669]' is not marked as EOG\n",
      "load: control token:    672 '[control_670]' is not marked as EOG\n",
      "load: control token:    673 '[control_671]' is not marked as EOG\n",
      "load: control token:    674 '[control_672]' is not marked as EOG\n",
      "load: control token:    675 '[control_673]' is not marked as EOG\n",
      "load: control token:    676 '[control_674]' is not marked as EOG\n",
      "load: control token:    677 '[control_675]' is not marked as EOG\n",
      "load: control token:    678 '[control_676]' is not marked as EOG\n",
      "load: control token:    679 '[control_677]' is not marked as EOG\n",
      "load: control token:    680 '[control_678]' is not marked as EOG\n",
      "load: control token:    681 '[control_679]' is not marked as EOG\n",
      "load: control token:    682 '[control_680]' is not marked as EOG\n",
      "load: control token:    683 '[control_681]' is not marked as EOG\n",
      "load: control token:    684 '[control_682]' is not marked as EOG\n",
      "load: control token:    685 '[control_683]' is not marked as EOG\n",
      "load: control token:    688 '[control_686]' is not marked as EOG\n",
      "load: control token:    689 '[control_687]' is not marked as EOG\n",
      "load: control token:    700 '[control_698]' is not marked as EOG\n",
      "load: control token:    701 '[control_699]' is not marked as EOG\n",
      "load: control token:    704 '[control_702]' is not marked as EOG\n",
      "load: control token:    705 '[control_703]' is not marked as EOG\n",
      "load: control token:    706 '[control_704]' is not marked as EOG\n",
      "load: control token:    707 '[control_705]' is not marked as EOG\n",
      "load: control token:    708 '[control_706]' is not marked as EOG\n",
      "load: control token:    709 '[control_707]' is not marked as EOG\n",
      "load: control token:    712 '[control_710]' is not marked as EOG\n",
      "load: control token:    713 '[control_711]' is not marked as EOG\n",
      "load: control token:    714 '[control_712]' is not marked as EOG\n",
      "load: control token:    715 '[control_713]' is not marked as EOG\n",
      "load: control token:    716 '[control_714]' is not marked as EOG\n",
      "load: control token:    717 '[control_715]' is not marked as EOG\n",
      "load: control token:    722 '[control_720]' is not marked as EOG\n",
      "load: control token:    723 '[control_721]' is not marked as EOG\n",
      "load: control token:    726 '[control_724]' is not marked as EOG\n",
      "load: control token:    727 '[control_725]' is not marked as EOG\n",
      "load: control token:    728 '[control_726]' is not marked as EOG\n",
      "load: control token:    729 '[control_727]' is not marked as EOG\n",
      "load: control token:    734 '[control_732]' is not marked as EOG\n",
      "load: control token:    735 '[control_733]' is not marked as EOG\n",
      "load: control token:    736 '[control_734]' is not marked as EOG\n",
      "load: control token:    737 '[control_735]' is not marked as EOG\n",
      "load: control token:    738 '[control_736]' is not marked as EOG\n",
      "load: control token:    739 '[control_737]' is not marked as EOG\n",
      "load: control token:    742 '[control_740]' is not marked as EOG\n",
      "load: control token:    743 '[control_741]' is not marked as EOG\n",
      "load: control token:    744 '[control_742]' is not marked as EOG\n",
      "load: control token:    745 '[control_743]' is not marked as EOG\n",
      "load: control token:    746 '[control_744]' is not marked as EOG\n",
      "load: control token:    747 '[control_745]' is not marked as EOG\n",
      "load: control token:    748 '[control_746]' is not marked as EOG\n",
      "load: control token:    749 '[control_747]' is not marked as EOG\n",
      "load: control token:    750 '[control_748]' is not marked as EOG\n",
      "load: control token:    751 '[control_749]' is not marked as EOG\n",
      "load: control token:    752 '[control_750]' is not marked as EOG\n",
      "load: control token:    753 '[control_751]' is not marked as EOG\n",
      "load: control token:    754 '[control_752]' is not marked as EOG\n",
      "load: control token:    755 '[control_753]' is not marked as EOG\n",
      "load: control token:    756 '[control_754]' is not marked as EOG\n",
      "load: control token:    757 '[control_755]' is not marked as EOG\n",
      "load: control token:    764 '[control_762]' is not marked as EOG\n",
      "load: control token:    765 '[control_763]' is not marked as EOG\n",
      "load: control token:    766 '[control_764]' is not marked as EOG\n",
      "load: control token:    767 '[control_765]' is not marked as EOG\n",
      "load: control token:    768 '[control_766]' is not marked as EOG\n",
      "load: control token:    769 '[control_767]' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 771\n",
      "load: token to piece cache size = 0.1731 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.25 B\n",
      "print_info: general.name     = Mistral-7B-Instruct-v0.3\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32768\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 781 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (iq4_xs) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  3730.02 MiB\n",
      "..................................................................................................\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 2048\n",
      "llama_init_from_model: n_ctx_per_seq = 2048\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 1000000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_init_from_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   164.01 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'Mistral-7B-Instruct-v0.3', 'general.architecture': 'llama', 'llama.block_count': '32', 'llama.context_length': '32768', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '30', 'llama.attention.head_count_kv': '8', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '1000000.000000', 'quantize.imatrix.entries_count': '224', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.vocab_size': '32768', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'llama', 'tokenizer.ggml.pre': 'default', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'quantize.imatrix.chunks_count': '228', 'quantize.imatrix.file': '/models/Mistral-7B-Instruct-v0.3-GGUF/Mistral-7B-Instruct-v0.3.imatrix', 'quantize.imatrix.dataset': '/training_data/calibration_data.txt'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "# === SETUP ===\n",
    "MODEL_PATH = \"./models/mistral_lite/Mistral-7B-Instruct-v0.3-IQ4_XS.gguf\"  # Adjust to your GGUF file\n",
    "CHUNK_SIZE = 1200  # tokens\n",
    "OVERLAP = 200      # tokens for overlap between chunks\n",
    "\n",
    "# === LOAD LLM ===\n",
    "llm = Llama(\n",
    "    model_path=MODEL_PATH,\n",
    "    n_gpu_layers=32,\n",
    "    n_ctx=2048,\n",
    "    n_threads=8,\n",
    "    use_mmap=True,\n",
    "    use_mlock=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TOKENIZER-LIKE CHUNKING ===\n",
    "def chunk_text(text, chunk_size=CHUNK_SIZE, overlap=OVERLAP):\n",
    "    import tiktoken  # Optional: llama-cpp-python has its own tokenizer too\n",
    "    tokenizer = llm.tokenize\n",
    "    tokens = tokenizer(text.encode(\"utf-8\"))[1:]  # Remove BOS\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(tokens), chunk_size - overlap):\n",
    "        chunk = tokens[i:i + chunk_size]\n",
    "        if chunk:\n",
    "            decoded = llm.detokenize(chunk).decode(\"utf-8\", errors=\"ignore\")\n",
    "            chunks.append(decoded)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SUMMARIZE ONE CHUNK ===\n",
    "def summarize_chunk(text):\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "You are tasked with summarizing a portion of a meeting transcript.\n",
    "Keep the summary detailed, reflecting key points, even if minor.\n",
    "Do not omit information you think might be relevant later.\n",
    "The summary should be no more than five sentences.\n",
    "This is just one part of the entire conversation.\n",
    "\n",
    "### Input:\n",
    "{text}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    result = llm(prompt, max_tokens=512, stop=[\"###\", \"### Instruction\", \"### Input\", \"### Response\"])\n",
    "    return result[\"choices\"][0][\"text\"].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GENERATE FINAL SUMMARY PROMPT ===\n",
    "def summarize_final_prompt(summaries):\n",
    "    joined = \"\\n\".join(summaries)\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "You are tasked with generating a concise and coherent summary from multiple segment summaries of a meeting.\n",
    "Ensure the final summary is comprehensive, relevant, and covers the entire meeting meaningfully.\n",
    "The final summary should be no more than ten sentences.\n",
    "Use the segment summaries below to infer key ideas and decisions made.\n",
    "\n",
    "### Segment Summaries:\n",
    "{joined}\n",
    "\n",
    "### Final Summary:\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_hierarchical(chunk_summaries, group_size=3):\n",
    "    grouped_summaries = [\n",
    "        chunk_summaries[i:i + group_size]\n",
    "        for i in range(0, len(chunk_summaries), group_size)\n",
    "    ]\n",
    "\n",
    "    intermediate_summaries = []\n",
    "    for i, group in enumerate(grouped_summaries):\n",
    "        joined = \"\\n\".join(group)\n",
    "        prompt = f\"\"\"### Instruction:\n",
    "You are tasked with summarizing a portion of a meeting transcript.\n",
    "Keep the summary detailed, reflecting key points, even if minor.\n",
    "Do not omit information you think might be relevant later.\n",
    "The summary should be no more than seven sentences.\n",
    "Use the segment summaries below to infer key ideas and decisions made.\n",
    "\n",
    "\n",
    "### Segment Summaries:\n",
    "{joined}\n",
    "\n",
    "### Group Summary:\n",
    "\"\"\"\n",
    "        print(f\"📝 Summarizing group {i + 1}\")\n",
    "        response = llm(prompt, max_tokens=256)\n",
    "        summary = response[\"choices\"][0][\"text\"].strip()\n",
    "        intermediate_summaries.append(summary)\n",
    "\n",
    "    return summarize_final_prompt(intermediate_summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MAIN SUMMARIZATION PIPELINE ===\n",
    "def summarize_transcript(full_text):\n",
    "    print(\"🔄 Chunking transcript...\")\n",
    "    chunks = chunk_text(full_text)\n",
    "    print(f\"✅ {len(chunks)} chunks created.\")\n",
    "\n",
    "    chunk_summaries = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"📝 Summarizing chunk {i+1}/{len(chunks)}...\")\n",
    "        summary = summarize_chunk(chunk)\n",
    "        chunk_summaries.append(summary)\n",
    "\n",
    "    print(\"\\n🧠 Generating final summary prompt...\")\n",
    "    final_prompt = summarize_final_prompt(chunk_summaries)\n",
    "\n",
    "    print(\"📚 Running final summary inference...\")\n",
    "    final_response = llm(final_prompt, max_tokens=512, stop=[\"###\", \"### Instruction\", \"### Segment Summaries\", \"### Final Summary\"])\n",
    "    final_summary = final_response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "    print(\"\\n✅ Final Summary:\\n\")\n",
    "    print(textwrap.fill(final_summary, 100))\n",
    "\n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === USAGE EXAMPLE ===\n",
    "# # Replace this with your transcript\n",
    "# with open(\"./amicorpus/ES2002c/ES2002c.Mix-Headset.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     full_transcript = f.read()\n",
    "\n",
    "# final_summary = summarize_transcript(full_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_meeting_folder(folder_path):\n",
    "    base_name = os.path.basename(folder_path)\n",
    "    transcript_path = os.path.join(folder_path, f\"{base_name}.Mix-Headset.txt\")\n",
    "    summary_path = os.path.join(folder_path, f\"{base_name}.summary.txt\")\n",
    "    \n",
    "    if os.path.exists(summary_path):\n",
    "        print(f\"[SKIP] Summary already exists: {summary_path}\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(transcript_path):\n",
    "        print(f\"[WARN] Transcript not found: {transcript_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"[INFO] Processing: {transcript_path}\")\n",
    "    \n",
    "    with open(transcript_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        transcript = f.read()\n",
    "        \n",
    "    final_summary = summarize_transcript(transcript)\n",
    "    \n",
    "    \n",
    "    output_path = os.path.join(folder_path, f\"{base_name}.summary.txt\")\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(final_summary)\n",
    "    \n",
    "    print(f\"[DONE] Summary saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP] Summary already exists: ./amicorpus/ES2002a\\ES2002a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2002b\\ES2002b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2002c\\ES2002c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2002d\\ES2002d.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2003a\\ES2003a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2003b\\ES2003b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2003c\\ES2003c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2003d\\ES2003d.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2004a\\ES2004a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2004b\\ES2004b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2004c\\ES2004c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2004d\\ES2004d.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2005a\\ES2005a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2005b\\ES2005b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2005c\\ES2005c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2005d\\ES2005d.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2006a\\ES2006a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2006b\\ES2006b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2006c\\ES2006c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2006d\\ES2006d.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2007a\\ES2007a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2007b\\ES2007b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2007c\\ES2007c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2007d\\ES2007d.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2008a\\ES2008a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2008b\\ES2008b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2008c\\ES2008c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2008d\\ES2008d.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2009a\\ES2009a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2009b\\ES2009b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2009c\\ES2009c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2009d\\ES2009d.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2010a\\ES2010a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2010b\\ES2010b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2010c\\ES2010c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2010d\\ES2010d.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2011a\\ES2011a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2011b\\ES2011b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2011c\\ES2011c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2011d\\ES2011d.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2012a\\ES2012a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2012b\\ES2012b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2012c\\ES2012c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2012d\\ES2012d.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2013a\\ES2013a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2013b\\ES2013b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2013c\\ES2013c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2013d\\ES2013d.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2014a\\ES2014a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2014b\\ES2014b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2014c\\ES2014c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2014d\\ES2014d.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2015a\\ES2015a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2015b\\ES2015b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2015c\\ES2015c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2015d\\ES2015d.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2016a\\ES2016a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2016b\\ES2016b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2016c\\ES2016c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/ES2016d\\ES2016d.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1000a\\IS1000a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1000b\\IS1000b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1000c\\IS1000c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1000d\\IS1000d.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1001a\\IS1001a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1001b\\IS1001b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1001c\\IS1001c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1001d\\IS1001d.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1002b\\IS1002b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1002c\\IS1002c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1002d\\IS1002d.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1003a\\IS1003a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1003b\\IS1003b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1003c\\IS1003c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1003d\\IS1003d.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1004a\\IS1004a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1004b\\IS1004b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1004c\\IS1004c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1004d\\IS1004d.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1005a\\IS1005a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1005b\\IS1005b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1005c\\IS1005c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1006a\\IS1006a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1006b\\IS1006b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1006c\\IS1006c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1006d\\IS1006d.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1007a\\IS1007a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1007b\\IS1007b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1007c\\IS1007c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1007d\\IS1007d.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1008a\\IS1008a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1008b\\IS1008b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1008c\\IS1008c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1008d\\IS1008d.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1009a\\IS1009a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1009b\\IS1009b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1009c\\IS1009c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/IS1009d\\IS1009d.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/TS3003a\\TS3003a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/TS3003b\\TS3003b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/TS3003c\\TS3003c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/TS3003d\\TS3003d.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/TS3004a\\TS3004a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/TS3004b\\TS3004b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/TS3004c\\TS3004c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/TS3004d\\TS3004d.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/TS3005a\\TS3005a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/TS3005b\\TS3005b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/TS3005c\\TS3005c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/TS3005d\\TS3005d.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/TS3006a\\TS3006a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/TS3006b\\TS3006b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/TS3006c\\TS3006c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/TS3006d\\TS3006d.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/TS3007a\\TS3007a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/TS3007b\\TS3007b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/TS3007c\\TS3007c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/TS3007d\\TS3007d.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/TS3008a\\TS3008a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/TS3008b\\TS3008b.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/TS3008c\\TS3008c.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/TS3008d\\TS3008d.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/TS3009a\\TS3009a.summary.txt\n",
      "[SKIP] Summary already exists: ./amicorpus/TS3009b\\TS3009b.summary.txt\n",
      "[INFO] Processing: ./amicorpus/TS3009c\\TS3009c.Mix-Headset.txt\n",
      "🔄 Chunking transcript...\n",
      "✅ 7 chunks created.\n",
      "📝 Summarizing chunk 1/7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  148898.54 ms /  1283 tokens (  116.05 ms per token,     8.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =   42630.65 ms /   245 runs   (  174.00 ms per token,     5.75 tokens per second)\n",
      "llama_perf_context_print:       total time =  191710.12 ms /  1528 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 2/7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  144503.44 ms /  1206 tokens (  119.82 ms per token,     8.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26635.79 ms /   153 runs   (  174.09 ms per token,     5.74 tokens per second)\n",
      "llama_perf_context_print:       total time =  171208.88 ms /  1359 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 3/7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145332.76 ms /  1206 tokens (  120.51 ms per token,     8.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =   29285.85 ms /   169 runs   (  173.29 ms per token,     5.77 tokens per second)\n",
      "llama_perf_context_print:       total time =  174698.18 ms /  1375 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 4/7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145323.61 ms /  1206 tokens (  120.50 ms per token,     8.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26392.94 ms /   149 runs   (  177.13 ms per token,     5.65 tokens per second)\n",
      "llama_perf_context_print:       total time =  171782.92 ms /  1355 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 5/7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  146082.68 ms /  1206 tokens (  121.13 ms per token,     8.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =   34959.89 ms /   200 runs   (  174.80 ms per token,     5.72 tokens per second)\n",
      "llama_perf_context_print:       total time =  181145.04 ms /  1406 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1187 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 6/7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  144344.78 ms /  1187 tokens (  121.60 ms per token,     8.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24171.47 ms /   139 runs   (  173.90 ms per token,     5.75 tokens per second)\n",
      "llama_perf_context_print:       total time =  168575.47 ms /  1326 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 187 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 7/7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =   22032.13 ms /   187 tokens (  117.82 ms per token,     8.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21286.03 ms /   130 runs   (  163.74 ms per token,     6.11 tokens per second)\n",
      "llama_perf_context_print:       total time =   43371.38 ms /   317 tokens\n",
      "Llama.generate: 11 prefix-match hit, remaining 1273 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Generating final summary prompt...\n",
      "📚 Running final summary inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  153294.41 ms /  1273 tokens (  120.42 ms per token,     8.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =   31250.39 ms /   180 runs   (  173.61 ms per token,     5.76 tokens per second)\n",
      "llama_perf_context_print:       total time =  184629.98 ms /  1453 tokens\n",
      "Llama.generate: 11 prefix-match hit, remaining 1272 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final Summary:\n",
      "\n",
      "During the meeting, the team discussed and agreed on the design of a new remote control, focusing on\n",
      "aesthetics, user-friendliness, and technological innovations. The design should target a younger\n",
      "demographic, featuring a colorful, round shape, and curved, rubber buttons. The remote will have an\n",
      "LCD screen, speech recognition, and a titanium case for a strong, iron-like feel. The design will\n",
      "avoid a fruity appearance and will be launched in five packages with basic colors. The user\n",
      "interface and packaging decisions will be made later. The team decided to work collaboratively on\n",
      "the design, with Mike, Thijs, and others involved in the decision-making process. Some concerns were\n",
      "raised about fairness regarding time off, and the location for design work was not decided. The team\n",
      "expressed enthusiasm for the project and agreed to continue working together.\n",
      "[DONE] Summary saved to: ./amicorpus/TS3009c\\TS3009c.summary.txt\n",
      "[INFO] Processing: ./amicorpus/TS3009d\\TS3009d.Mix-Headset.txt\n",
      "🔄 Chunking transcript...\n",
      "✅ 7 chunks created.\n",
      "📝 Summarizing chunk 1/7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  153117.14 ms /  1272 tokens (  120.38 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   31050.43 ms /   179 runs   (  173.47 ms per token,     5.76 tokens per second)\n",
      "llama_perf_context_print:       total time =  184253.60 ms /  1451 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 2/7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145566.18 ms /  1206 tokens (  120.70 ms per token,     8.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26441.81 ms /   151 runs   (  175.11 ms per token,     5.71 tokens per second)\n",
      "llama_perf_context_print:       total time =  172074.67 ms /  1357 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 3/7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145432.29 ms /  1206 tokens (  120.59 ms per token,     8.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =   37556.94 ms /   216 runs   (  173.87 ms per token,     5.75 tokens per second)\n",
      "llama_perf_context_print:       total time =  183104.54 ms /  1422 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 4/7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145463.34 ms /  1206 tokens (  120.62 ms per token,     8.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =   34287.15 ms /   197 runs   (  174.05 ms per token,     5.75 tokens per second)\n",
      "llama_perf_context_print:       total time =  179849.03 ms /  1403 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 5/7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145466.65 ms /  1206 tokens (  120.62 ms per token,     8.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =   32895.14 ms /   189 runs   (  174.05 ms per token,     5.75 tokens per second)\n",
      "llama_perf_context_print:       total time =  178454.58 ms /  1395 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1169 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 6/7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  140470.82 ms /  1169 tokens (  120.16 ms per token,     8.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24478.19 ms /   141 runs   (  173.60 ms per token,     5.76 tokens per second)\n",
      "llama_perf_context_print:       total time =  165010.18 ms /  1310 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 169 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 7/7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =   19855.25 ms /   169 tokens (  117.49 ms per token,     8.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =   17052.31 ms /   104 runs   (  163.96 ms per token,     6.10 tokens per second)\n",
      "llama_perf_context_print:       total time =   36947.11 ms /   273 tokens\n",
      "Llama.generate: 11 prefix-match hit, remaining 1266 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Generating final summary prompt...\n",
      "📚 Running final summary inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  152262.04 ms /  1266 tokens (  120.27 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   50020.80 ms /   286 runs   (  174.90 ms per token,     5.72 tokens per second)\n",
      "llama_perf_context_print:       total time =  202459.33 ms /  1552 tokens\n",
      "Llama.generate: 11 prefix-match hit, remaining 1121 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final Summary:\n",
      "\n",
      "In a series of meetings, the team discussed and refined the design of a device, focusing on its\n",
      "appearance, ergonomics, and cost. The device features a round front with a hard plastic design,\n",
      "basic colors, and potential patterns and pictures. The back, primarily titanium-colored plastic,\n",
      "will house the company logo, LCD screen, channel, volume controls, buttons for menu, video, and\n",
      "voice recorder. The team agreed on the device's overall shape and size, with comfort in mind.\n",
      "Concerns were raised about the cost, specifically the sample speaker and the advanced chip for the\n",
      "LCD screen. To reduce costs, the team decided to replace titanium with titanium-colored plastic,\n",
      "double the use of plastic, and add a special color to the design. The team is also maintaining a\n",
      "budget for gold plating and chrome additions. The group acknowledged the need to balance cost and\n",
      "design, expressing dissatisfaction with their account managers for revealing unexpected costs at the\n",
      "last moment. They also discussed various meeting tools, with a preference for a green board with a\n",
      "digital pen over smart boards and large televisions. The team is keeping laptops but dislikes the\n",
      "cameras. The final document title was decided as \"We put the fashion in electronics, but we\n",
      "couldn't.\" One team member plans to resign after the project's completion.\n",
      "[DONE] Summary saved to: ./amicorpus/TS3009d\\TS3009d.summary.txt\n",
      "[INFO] Processing: ./amicorpus/TS3010a\\TS3010a.Mix-Headset.txt\n",
      "🔄 Chunking transcript...\n",
      "✅ 2 chunks created.\n",
      "📝 Summarizing chunk 1/2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  135282.50 ms /  1121 tokens (  120.68 ms per token,     8.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =   30410.43 ms /   176 runs   (  172.79 ms per token,     5.79 tokens per second)\n",
      "llama_perf_context_print:       total time =  165777.15 ms /  1297 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 55 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 2/2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =    6478.30 ms /    55 tokens (  117.79 ms per token,     8.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7261.95 ms /    45 runs   (  161.38 ms per token,     6.20 tokens per second)\n",
      "llama_perf_context_print:       total time =   13754.16 ms /   100 tokens\n",
      "Llama.generate: 11 prefix-match hit, remaining 302 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Generating final summary prompt...\n",
      "📚 Running final summary inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =   35660.28 ms /   302 tokens (  118.08 ms per token,     8.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =   23559.37 ms /   143 runs   (  164.75 ms per token,     6.07 tokens per second)\n",
      "llama_perf_context_print:       total time =   59280.08 ms /   445 tokens\n",
      "Llama.generate: 11 prefix-match hit, remaining 1272 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final Summary:\n",
      "\n",
      "In the initial meeting, the team discussed and agreed to create a new, aesthetically pleasing, and\n",
      "functional remote control. Utilizing a drawing tool, they brainstormed ideas for a non-breakable\n",
      "design, larger buttons, audible feedback, a battery level indicator, and matching aesthetics with\n",
      "existing products. The project was capped at 12.5 euros and participants shared their concerns\n",
      "regarding existing remote control issues, such as usability, breakability, and lack of clear\n",
      "indicators. The team was instructed to continue their research and await further instructions via\n",
      "email, with the next meeting scheduled in half an hour. The meeting concluded amicably with standard\n",
      "farewells.\n",
      "[DONE] Summary saved to: ./amicorpus/TS3010a\\TS3010a.summary.txt\n",
      "[INFO] Processing: ./amicorpus/TS3010b\\TS3010b.Mix-Headset.txt\n",
      "🔄 Chunking transcript...\n",
      "✅ 4 chunks created.\n",
      "📝 Summarizing chunk 1/4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  153544.11 ms /  1272 tokens (  120.71 ms per token,     8.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =   22369.47 ms /   128 runs   (  174.76 ms per token,     5.72 tokens per second)\n",
      "llama_perf_context_print:       total time =  175966.46 ms /  1400 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 2/4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145397.86 ms /  1206 tokens (  120.56 ms per token,     8.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =   37743.93 ms /   217 runs   (  173.94 ms per token,     5.75 tokens per second)\n",
      "llama_perf_context_print:       total time =  183255.25 ms /  1423 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 3/4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145209.01 ms /  1206 tokens (  120.41 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   37957.96 ms /   216 runs   (  175.73 ms per token,     5.69 tokens per second)\n",
      "llama_perf_context_print:       total time =  183280.01 ms /  1422 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 466 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 4/4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =   55509.18 ms /   466 tokens (  119.12 ms per token,     8.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =   17006.83 ms /   101 runs   (  168.38 ms per token,     5.94 tokens per second)\n",
      "llama_perf_context_print:       total time =   72554.17 ms /   567 tokens\n",
      "Llama.generate: 11 prefix-match hit, remaining 745 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Generating final summary prompt...\n",
      "📚 Running final summary inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =   88879.96 ms /   745 tokens (  119.30 ms per token,     8.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =   50564.56 ms /   295 runs   (  171.41 ms per token,     5.83 tokens per second)\n",
      "llama_perf_context_print:       total time =  139633.16 ms /  1040 tokens\n",
      "Llama.generate: 11 prefix-match hit, remaining 1272 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final Summary:\n",
      "\n",
      "In a 40-minute meeting, the team discussed the design of a new remote control for a television. The\n",
      "designer proposed a design featuring a single long-lasting battery, minimal buttons, plastic\n",
      "construction, and compatibility with various televisions. A trendy design with interchangeable\n",
      "covers was suggested, but its potential complexity and impact on profit were debated. The remote was\n",
      "to have basic functions such as changing channels, volume, and power, or more advanced functions\n",
      "that could influence other devices. A usability lab test revealed that users disliked the current\n",
      "look and feel of remote controls, found them hard to learn, and frequently used only a small\n",
      "percentage of the buttons. The team identified two main market segments: the younger group between\n",
      "16 and 45, who are interested in new features and critical about spending, and the older group\n",
      "between 46 and 65, who are less interested in new features. The team is still deciding whether to\n",
      "focus on a basic remote control or one with more advanced functions, and they agreed on the\n",
      "importance of designing an easy-to-use, aesthetically pleasing remote control. The team also agreed\n",
      "on the necessity of teletext, power, separate channels, volume control, a menu, and a mute button.\n",
      "The design phase will be addressed in a subsequent meeting, and minutes from the meeting can be\n",
      "found in the project document folder. The meeting was followed by a lunch break.\n",
      "[DONE] Summary saved to: ./amicorpus/TS3010b\\TS3010b.summary.txt\n",
      "[INFO] Processing: ./amicorpus/TS3010c\\TS3010c.Mix-Headset.txt\n",
      "🔄 Chunking transcript...\n",
      "✅ 4 chunks created.\n",
      "📝 Summarizing chunk 1/4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  153626.96 ms /  1272 tokens (  120.78 ms per token,     8.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =   32153.96 ms /   184 runs   (  174.75 ms per token,     5.72 tokens per second)\n",
      "llama_perf_context_print:       total time =  185869.43 ms /  1456 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 2/4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145260.66 ms /  1206 tokens (  120.45 ms per token,     8.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =   34434.41 ms /   198 runs   (  173.91 ms per token,     5.75 tokens per second)\n",
      "llama_perf_context_print:       total time =  179795.33 ms /  1404 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 3/4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145759.63 ms /  1206 tokens (  120.86 ms per token,     8.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21670.83 ms /   125 runs   (  173.37 ms per token,     5.77 tokens per second)\n",
      "llama_perf_context_print:       total time =  167480.63 ms /  1331 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 806 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 4/4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =   96148.21 ms /   806 tokens (  119.29 ms per token,     8.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24306.12 ms /   143 runs   (  169.97 ms per token,     5.88 tokens per second)\n",
      "llama_perf_context_print:       total time =  120515.02 ms /   949 tokens\n",
      "Llama.generate: 11 prefix-match hit, remaining 733 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Generating final summary prompt...\n",
      "📚 Running final summary inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =   87235.35 ms /   733 tokens (  119.01 ms per token,     8.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26378.02 ms /   153 runs   (  172.41 ms per token,     5.80 tokens per second)\n",
      "llama_perf_context_print:       total time =  113681.20 ms /   886 tokens\n",
      "Llama.generate: 11 prefix-match hit, remaining 1272 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final Summary:\n",
      "\n",
      "The group is designing a remote control for televisions, focusing on a hand-shaped, primary-colored\n",
      "design with a scroll push button mechanism, larger, curved buttons, and a potential switch system.\n",
      "The remote control will likely use standard or kinetic batteries, and will not include an LCD screen\n",
      "due to cost concerns. The team is open to exploring other concepts, such as a kinetic design, but\n",
      "prefers a normal battery for cost reasons. The remote control is intended to be appealing to younger\n",
      "users, and the team is considering a softer, squeezable structure for easier use. The design and\n",
      "cost will be finalized in the next meeting, with potential consideration of a joystick for\n",
      "navigation. The meeting was conducted in Dutch.\n",
      "[DONE] Summary saved to: ./amicorpus/TS3010c\\TS3010c.summary.txt\n",
      "[INFO] Processing: ./amicorpus/TS3010d\\TS3010d.Mix-Headset.txt\n",
      "🔄 Chunking transcript...\n",
      "✅ 3 chunks created.\n",
      "📝 Summarizing chunk 1/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  153103.76 ms /  1272 tokens (  120.36 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   46458.21 ms /   266 runs   (  174.65 ms per token,     5.73 tokens per second)\n",
      "llama_perf_context_print:       total time =  199718.97 ms /  1538 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 2/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145282.96 ms /  1206 tokens (  120.47 ms per token,     8.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28725.78 ms /   165 runs   (  174.10 ms per token,     5.74 tokens per second)\n",
      "llama_perf_context_print:       total time =  174084.01 ms /  1371 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 981 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 3/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  117990.80 ms /   981 tokens (  120.28 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28337.54 ms /   165 runs   (  171.74 ms per token,     5.82 tokens per second)\n",
      "llama_perf_context_print:       total time =  146403.48 ms /  1146 tokens\n",
      "Llama.generate: 11 prefix-match hit, remaining 678 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Generating final summary prompt...\n",
      "📚 Running final summary inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =   80465.14 ms /   678 tokens (  118.68 ms per token,     8.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28604.80 ms /   169 runs   (  169.26 ms per token,     5.91 tokens per second)\n",
      "llama_perf_context_print:       total time =  109148.17 ms /   847 tokens\n",
      "Llama.generate: 11 prefix-match hit, remaining 1272 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final Summary:\n",
      "\n",
      "The group discussed and evaluated a remote control design aimed at the younger market. The remote\n",
      "features big, color-coded buttons, a scroll button, and a joystick for volume and channel control.\n",
      "It was given mixed ratings for design and fashion appeal, with a focus on minimizing button count\n",
      "and appealing to the target demographic. The remote was initially priced at 13 euros, but the\n",
      "decision was made to remove the joystick and reduce the price to 12.5 euros. The team found the\n",
      "design process challenging but productive, using a smart board for brainstorming. A questionnaire\n",
      "will be used for final feedback, and the group felt the project was progressing well and ready for\n",
      "further reevaluation. The meeting concluded on a positive note, looking forward to moving forward\n",
      "with the project.\n",
      "[DONE] Summary saved to: ./amicorpus/TS3010d\\TS3010d.summary.txt\n",
      "[INFO] Processing: ./amicorpus/TS3011a\\TS3011a.Mix-Headset.txt\n",
      "🔄 Chunking transcript...\n",
      "✅ 3 chunks created.\n",
      "📝 Summarizing chunk 1/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  152881.01 ms /  1272 tokens (  120.19 ms per token,     8.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =   31301.28 ms /   179 runs   (  174.87 ms per token,     5.72 tokens per second)\n",
      "llama_perf_context_print:       total time =  184267.76 ms /  1451 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 2/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145230.01 ms /  1206 tokens (  120.42 ms per token,     8.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26912.90 ms /   154 runs   (  174.76 ms per token,     5.72 tokens per second)\n",
      "llama_perf_context_print:       total time =  172211.73 ms /  1360 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 780 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 3/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =   92958.33 ms /   780 tokens (  119.18 ms per token,     8.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21784.85 ms /   129 runs   (  168.87 ms per token,     5.92 tokens per second)\n",
      "llama_perf_context_print:       total time =  114797.37 ms /   909 tokens\n",
      "Llama.generate: 11 prefix-match hit, remaining 544 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Generating final summary prompt...\n",
      "📚 Running final summary inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =   64782.75 ms /   544 tokens (  119.09 ms per token,     8.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =   38380.17 ms /   229 runs   (  167.60 ms per token,     5.97 tokens per second)\n",
      "llama_perf_context_print:       total time =  103288.36 ms /   773 tokens\n",
      "Llama.generate: 11 prefix-match hit, remaining 1272 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final Summary:\n",
      "\n",
      "The team held a meeting today to discuss the development of a trendy, user-friendly, and\n",
      "internationally marketed universal remote control device. The project will progress through multiple\n",
      "design stages, using tools like presentation software and an electronic whiteboard system for\n",
      "collaboration. The device is expected to support various equipment such as DVD players, cell phones,\n",
      "video, and audio equipment, and will be priced at approximately 25 euros, with production costs not\n",
      "exceeding 12.50 euros. To ensure user-friendliness and compatibility, the industrial designer will\n",
      "focus on technical possibilities and limitations, user interface, and interoperability, suggesting a\n",
      "touchscreen or control stick for easy navigation. The group also discussed potential design aspects,\n",
      "including incorporating an infrared system, ensuring visibility in dark conditions, and the\n",
      "possibility of including a display. Concerns about Bluetooth power consumption and button paint\n",
      "fading over time were raised, with suggestions for rechargeable batteries or power-saving features.\n",
      "The meeting ended with reminders about the next gathering and the importance of the project\n",
      "targeting countries with market interest.\n",
      "[DONE] Summary saved to: ./amicorpus/TS3011a\\TS3011a.summary.txt\n",
      "[INFO] Processing: ./amicorpus/TS3011b\\TS3011b.Mix-Headset.txt\n",
      "🔄 Chunking transcript...\n",
      "✅ 6 chunks created.\n",
      "📝 Summarizing chunk 1/6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  153330.66 ms /  1272 tokens (  120.54 ms per token,     8.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =   32212.65 ms /   185 runs   (  174.12 ms per token,     5.74 tokens per second)\n",
      "llama_perf_context_print:       total time =  185632.27 ms /  1457 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 2/6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145738.31 ms /  1206 tokens (  120.84 ms per token,     8.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21116.77 ms /   122 runs   (  173.09 ms per token,     5.78 tokens per second)\n",
      "llama_perf_context_print:       total time =  166903.84 ms /  1328 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 3/6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145135.44 ms /  1206 tokens (  120.34 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21192.42 ms /   121 runs   (  175.14 ms per token,     5.71 tokens per second)\n",
      "llama_perf_context_print:       total time =  166376.56 ms /  1327 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 4/6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145526.53 ms /  1206 tokens (  120.67 ms per token,     8.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =   22708.91 ms /   131 runs   (  173.35 ms per token,     5.77 tokens per second)\n",
      "llama_perf_context_print:       total time =  168289.95 ms /  1337 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1199 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 5/6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  144202.03 ms /  1199 tokens (  120.27 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   27342.74 ms /   155 runs   (  176.40 ms per token,     5.67 tokens per second)\n",
      "llama_perf_context_print:       total time =  171614.67 ms /  1354 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 199 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 6/6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =   23869.13 ms /   199 tokens (  119.95 ms per token,     8.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =   13650.44 ms /    83 runs   (  164.46 ms per token,     6.08 tokens per second)\n",
      "llama_perf_context_print:       total time =   37548.77 ms /   282 tokens\n",
      "Llama.generate: 11 prefix-match hit, remaining 883 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Generating final summary prompt...\n",
      "📚 Running final summary inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  105261.12 ms /   883 tokens (  119.21 ms per token,     8.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =   29776.60 ms /   175 runs   (  170.15 ms per token,     5.88 tokens per second)\n",
      "llama_perf_context_print:       total time =  135121.10 ms /  1058 tokens\n",
      "Llama.generate: 11 prefix-match hit, remaining 1272 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final Summary:\n",
      "\n",
      "The meeting focused on designing a user-friendly remote control for customers under 40, prioritizing\n",
      "simplicity and energy efficiency. The remote control will communicate with TVs using infrared\n",
      "pulses, with a focus on essential functions such as volume and Z buttons. Advanced features like\n",
      "speech recognition and 10 digits are under consideration but prioritizing simplicity for cost\n",
      "efficiency. A round button will replace the traditional stick for volume and channel selection. The\n",
      "team is exploring a clapping technique for locating the remote control and a base station as\n",
      "alternatives to a more complex design with innovative functions. A simple user interface is being\n",
      "designed, with updates on voice recognition costs expected in the next meeting, where further\n",
      "decisions will be made. Roel will design the user interface, and Sebastian will decide on the voice\n",
      "recognition feature. The team agreed to have lunch after the meeting.\n",
      "[DONE] Summary saved to: ./amicorpus/TS3011b\\TS3011b.summary.txt\n",
      "[INFO] Processing: ./amicorpus/TS3011c\\TS3011c.Mix-Headset.txt\n",
      "🔄 Chunking transcript...\n",
      "✅ 6 chunks created.\n",
      "📝 Summarizing chunk 1/6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  153413.36 ms /  1272 tokens (  120.61 ms per token,     8.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21095.92 ms /   121 runs   (  174.35 ms per token,     5.74 tokens per second)\n",
      "llama_perf_context_print:       total time =  174558.49 ms /  1393 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 2/6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145272.04 ms /  1206 tokens (  120.46 ms per token,     8.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =   30256.67 ms /   174 runs   (  173.89 ms per token,     5.75 tokens per second)\n",
      "llama_perf_context_print:       total time =  175609.68 ms /  1380 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 3/6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  146702.75 ms /  1206 tokens (  121.64 ms per token,     8.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =   37222.05 ms /   214 runs   (  173.93 ms per token,     5.75 tokens per second)\n",
      "llama_perf_context_print:       total time =  184037.29 ms /  1420 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 4/6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145405.50 ms /  1206 tokens (  120.57 ms per token,     8.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =   29557.60 ms /   170 runs   (  173.87 ms per token,     5.75 tokens per second)\n",
      "llama_perf_context_print:       total time =  175042.17 ms /  1376 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 5/6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145377.61 ms /  1206 tokens (  120.55 ms per token,     8.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =   32504.92 ms /   187 runs   (  173.82 ms per token,     5.75 tokens per second)\n",
      "llama_perf_context_print:       total time =  177973.84 ms /  1393 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 465 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 6/6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =   55245.69 ms /   465 tokens (  118.81 ms per token,     8.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =   15238.68 ms /    91 runs   (  167.46 ms per token,     5.97 tokens per second)\n",
      "llama_perf_context_print:       total time =   70517.29 ms /   556 tokens\n",
      "Llama.generate: 11 prefix-match hit, remaining 1042 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Generating final summary prompt...\n",
      "📚 Running final summary inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  124850.66 ms /  1042 tokens (  119.82 ms per token,     8.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =   44098.49 ms /   255 runs   (  172.94 ms per token,     5.78 tokens per second)\n",
      "llama_perf_context_print:       total time =  169096.29 ms /  1297 tokens\n",
      "Llama.generate: 11 prefix-match hit, remaining 1272 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final Summary:\n",
      "\n",
      "The team met to discuss the design and functionality of a new remote control product. Preferences\n",
      "for the younger target group were identified, including a preference for colorful and soft\n",
      "materials, with innovation being more important than ease of use. The remote control design\n",
      "considered various shapes, such as an oval, curved, or three-dimensional form, and materials,\n",
      "including rubber, plastic, wood, and titanium. The design also incorporated a voice recognition\n",
      "feature, but challenges were encountered during brainstorming. Key decisions made during the meeting\n",
      "included the use of a single curved case, the combination of traditional batteries and solar energy\n",
      "as power sources, and the rubber finishing of the case in the company colors of grey and yellow. The\n",
      "user interface will be designed in the next phase, with the ID and UID teams working together on the\n",
      "prototype drawing. The scroll wheel, voice recognition, and spongy buttons were confirmed for the\n",
      "design. The team also decided to use regular chips for control, obtained water line information from\n",
      "the air conditioner, and left some decisions, such as the color scheme and feasibility of printing\n",
      "images, to be figured out by two team members. The meeting ended with one member confirming that\n",
      "they had not written down all the decisions made.\n",
      "[DONE] Summary saved to: ./amicorpus/TS3011c\\TS3011c.summary.txt\n",
      "[INFO] Processing: ./amicorpus/TS3011d\\TS3011d.Mix-Headset.txt\n",
      "🔄 Chunking transcript...\n",
      "✅ 5 chunks created.\n",
      "📝 Summarizing chunk 1/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  153278.48 ms /  1272 tokens (  120.50 ms per token,     8.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =   48139.14 ms /   270 runs   (  178.29 ms per token,     5.61 tokens per second)\n",
      "llama_perf_context_print:       total time =  201580.88 ms /  1542 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 2/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145459.17 ms /  1206 tokens (  120.61 ms per token,     8.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =   32206.60 ms /   184 runs   (  175.04 ms per token,     5.71 tokens per second)\n",
      "llama_perf_context_print:       total time =  177755.07 ms /  1390 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 3/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145024.24 ms /  1206 tokens (  120.25 ms per token,     8.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =   22868.16 ms /   130 runs   (  175.91 ms per token,     5.68 tokens per second)\n",
      "llama_perf_context_print:       total time =  167946.40 ms /  1336 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1193 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 4/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  143607.10 ms /  1193 tokens (  120.37 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   19529.75 ms /   112 runs   (  174.37 ms per token,     5.73 tokens per second)\n",
      "llama_perf_context_print:       total time =  163180.48 ms /  1305 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 193 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 5/5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =   22837.79 ms /   193 tokens (  118.33 ms per token,     8.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =   13998.25 ms /    86 runs   (  162.77 ms per token,     6.14 tokens per second)\n",
      "llama_perf_context_print:       total time =   36866.88 ms /   279 tokens\n",
      "Llama.generate: 11 prefix-match hit, remaining 866 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Generating final summary prompt...\n",
      "📚 Running final summary inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  103094.04 ms /   866 tokens (  119.05 ms per token,     8.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28407.13 ms /   166 runs   (  171.13 ms per token,     5.84 tokens per second)\n",
      "llama_perf_context_print:       total time =  131577.84 ms /  1032 tokens\n",
      "Llama.generate: 11 prefix-match hit, remaining 1272 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final Summary:\n",
      "\n",
      "The group discussed a remote control device design, focusing on user-friendly features, aesthetics,\n",
      "and potential cost reductions. The device features a round design with a scroll bar, programmable\n",
      "buttons, and a color scheme of yellow with gray or black. However, concerns were raised about high\n",
      "production costs, prompting the consideration of simpler materials. The device's innovative\n",
      "features, such as voice recognition, were acknowledged, but its energy source was criticized. The\n",
      "device's aesthetics were rated 2 out of 5, with suggestions for a titanium housing and a different\n",
      "color scheme. The meeting concluded with a discussion about a possible redesign, a champagne\n",
      "celebration, and criticisms of the smart board and digital pen. A new feature, the Bluetooth disable\n",
      "function, was also proposed.\n",
      "[DONE] Summary saved to: ./amicorpus/TS3011d\\TS3011d.summary.txt\n",
      "[INFO] Processing: ./amicorpus/TS3012a\\TS3012a.Mix-Headset.txt\n",
      "🔄 Chunking transcript...\n",
      "✅ 3 chunks created.\n",
      "📝 Summarizing chunk 1/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  153450.73 ms /  1272 tokens (  120.64 ms per token,     8.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =   31472.07 ms /   181 runs   (  173.88 ms per token,     5.75 tokens per second)\n",
      "llama_perf_context_print:       total time =  185010.46 ms /  1453 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1126 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 2/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  138270.47 ms /  1126 tokens (  122.80 ms per token,     8.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =   31432.05 ms /   135 runs   (  232.83 ms per token,     4.29 tokens per second)\n",
      "llama_perf_context_print:       total time =  169764.48 ms /  1261 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 126 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 3/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =   16092.78 ms /   126 tokens (  127.72 ms per token,     7.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =   17703.70 ms /    83 runs   (  213.30 ms per token,     4.69 tokens per second)\n",
      "llama_perf_context_print:       total time =   33827.46 ms /   209 tokens\n",
      "Llama.generate: 11 prefix-match hit, remaining 481 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Generating final summary prompt...\n",
      "📚 Running final summary inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =   57278.58 ms /   481 tokens (  119.08 ms per token,     8.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =   50525.62 ms /   302 runs   (  167.30 ms per token,     5.98 tokens per second)\n",
      "llama_perf_context_print:       total time =  107999.26 ms /   783 tokens\n",
      "Llama.generate: 11 prefix-match hit, remaining 1272 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final Summary:\n",
      "\n",
      "In the team meeting, Mark Knopf, the project manager, introduced the agenda, which revolved around\n",
      "designing a user-friendly remote control for a television. The team, consisting of a marketing\n",
      "expert, a user interface designer, and an industrial designer, will approach the project in three\n",
      "steps: functional, aesthetic, and finalization. The team was encouraged to share ideas and use a\n",
      "smart board for illustrations. A fun exercise stimulating creativity involved everyone drawing their\n",
      "favorite animal. The marketing expert chose an elephant, while the industrial designer drew a\n",
      "dolphin, with a suggestion for the latter to create an elephant design for the remote control.\n",
      "Meanwhile, a separate group discussed a multi-remote control for TV, focusing on design, usability,\n",
      "accessibility, and material. The consensus was that it should be intuitive, familiar, lightweight,\n",
      "and accessible to a wide audience, including people with disabilities. The proposed selling price\n",
      "was 25 euros, with a production cost of 12.5 euros. They agreed to reconvene in 30 minutes to share\n",
      "marketing trend ideas.  Lastly, another meeting focused on marketing trends, with personal\n",
      "assignments given by the coach. The meeting would reconvene in 30 minutes for the sharing of ideas\n",
      "generated. The meetings ended politely with the speaker reminding everyone to attend subsequent\n",
      "meetings. Some frustration was expressed about the smart board's responsiveness during the drawing\n",
      "exercise.\n",
      "[DONE] Summary saved to: ./amicorpus/TS3012a\\TS3012a.summary.txt\n",
      "[INFO] Processing: ./amicorpus/TS3012b\\TS3012b.Mix-Headset.txt\n",
      "🔄 Chunking transcript...\n",
      "✅ 8 chunks created.\n",
      "📝 Summarizing chunk 1/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  153237.69 ms /  1272 tokens (  120.47 ms per token,     8.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =   30952.17 ms /   178 runs   (  173.89 ms per token,     5.75 tokens per second)\n",
      "llama_perf_context_print:       total time =  184273.96 ms /  1450 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 2/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  146916.24 ms /  1206 tokens (  121.82 ms per token,     8.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21560.31 ms /   124 runs   (  173.87 ms per token,     5.75 tokens per second)\n",
      "llama_perf_context_print:       total time =  168526.91 ms /  1330 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 3/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145450.33 ms /  1206 tokens (  120.61 ms per token,     8.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =   39815.11 ms /   228 runs   (  174.63 ms per token,     5.73 tokens per second)\n",
      "llama_perf_context_print:       total time =  185388.49 ms /  1434 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 4/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145763.51 ms /  1206 tokens (  120.87 ms per token,     8.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =   32106.41 ms /   184 runs   (  174.49 ms per token,     5.73 tokens per second)\n",
      "llama_perf_context_print:       total time =  177959.21 ms /  1390 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 5/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145257.15 ms /  1206 tokens (  120.45 ms per token,     8.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26887.88 ms /   155 runs   (  173.47 ms per token,     5.76 tokens per second)\n",
      "llama_perf_context_print:       total time =  172214.19 ms /  1361 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 6/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145400.66 ms /  1206 tokens (  120.56 ms per token,     8.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =   30577.73 ms /   176 runs   (  173.74 ms per token,     5.76 tokens per second)\n",
      "llama_perf_context_print:       total time =  176061.18 ms /  1382 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 7/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145861.09 ms /  1206 tokens (  120.95 ms per token,     8.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24974.29 ms /   143 runs   (  174.65 ms per token,     5.73 tokens per second)\n",
      "llama_perf_context_print:       total time =  170897.29 ms /  1349 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 741 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 8/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =   88254.75 ms /   741 tokens (  119.10 ms per token,     8.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =   17837.69 ms /   106 runs   (  168.28 ms per token,     5.94 tokens per second)\n",
      "llama_perf_context_print:       total time =  106133.00 ms /   847 tokens\n",
      "Llama.generate: 11 prefix-match hit, remaining 1381 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Generating final summary prompt...\n",
      "📚 Running final summary inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  166987.33 ms /  1381 tokens (  120.92 ms per token,     8.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =   35842.04 ms /   204 runs   (  175.70 ms per token,     5.69 tokens per second)\n",
      "llama_perf_context_print:       total time =  202933.41 ms /  1585 tokens\n",
      "Llama.generate: 11 prefix-match hit, remaining 1272 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final Summary:\n",
      "\n",
      "The team discussed the redesign of a remote control, focusing on a user-friendly layout, durability,\n",
      "and a potential sound feature. Proposed designs include a remote with changeable covers, a\n",
      "distinctive shape, and a rechargeable battery. The remote is intended to appeal to a wide audience\n",
      "under 40, with a focus on fashion and functionality. Key features include simplified buttons for\n",
      "basic functions, an LCD screen for easy navigation (subject to cost and space availability), and a\n",
      "potential beep or flash feature for locating the remote. The team agreed on the importance of a\n",
      "futuristic design, easy accessibility of basic functions, and research into the costs and space\n",
      "requirements for the LCD and speech recognition features. The team is also considering making the\n",
      "remote smaller but possibly thicker to accommodate necessary electronics. The remote's design will\n",
      "be further deliberated, and the team will research the costs and possibilities of the LCD feature.\n",
      "The minutes of the meeting will be published online.\n",
      "[DONE] Summary saved to: ./amicorpus/TS3012b\\TS3012b.summary.txt\n",
      "[INFO] Processing: ./amicorpus/TS3012c\\TS3012c.Mix-Headset.txt\n",
      "🔄 Chunking transcript...\n",
      "✅ 8 chunks created.\n",
      "📝 Summarizing chunk 1/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  154143.55 ms /  1272 tokens (  121.18 ms per token,     8.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =   18801.68 ms /   107 runs   (  175.72 ms per token,     5.69 tokens per second)\n",
      "llama_perf_context_print:       total time =  172986.38 ms /  1379 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 2/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145143.93 ms /  1206 tokens (  120.35 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28443.14 ms /   162 runs   (  175.57 ms per token,     5.70 tokens per second)\n",
      "llama_perf_context_print:       total time =  173660.56 ms /  1368 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 3/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145147.51 ms /  1206 tokens (  120.35 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   31918.09 ms /   184 runs   (  173.47 ms per token,     5.76 tokens per second)\n",
      "llama_perf_context_print:       total time =  177154.57 ms /  1390 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 4/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145153.65 ms /  1206 tokens (  120.36 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   41576.05 ms /   239 runs   (  173.96 ms per token,     5.75 tokens per second)\n",
      "llama_perf_context_print:       total time =  186862.72 ms /  1445 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 5/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145377.54 ms /  1206 tokens (  120.55 ms per token,     8.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =   31286.81 ms /   179 runs   (  174.79 ms per token,     5.72 tokens per second)\n",
      "llama_perf_context_print:       total time =  176749.17 ms /  1385 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 6/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145454.20 ms /  1206 tokens (  120.61 ms per token,     8.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =   31607.91 ms /   181 runs   (  174.63 ms per token,     5.73 tokens per second)\n",
      "llama_perf_context_print:       total time =  177149.02 ms /  1387 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 7/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145144.32 ms /  1206 tokens (  120.35 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28736.66 ms /   164 runs   (  175.22 ms per token,     5.71 tokens per second)\n",
      "llama_perf_context_print:       total time =  173956.26 ms /  1370 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 241 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 8/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =   28383.50 ms /   241 tokens (  117.77 ms per token,     8.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =   14981.15 ms /    92 runs   (  162.84 ms per token,     6.14 tokens per second)\n",
      "llama_perf_context_print:       total time =   43398.35 ms /   333 tokens\n",
      "Llama.generate: 11 prefix-match hit, remaining 1395 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Generating final summary prompt...\n",
      "📚 Running final summary inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  168428.34 ms /  1395 tokens (  120.74 ms per token,     8.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =   39184.67 ms /   223 runs   (  175.72 ms per token,     5.69 tokens per second)\n",
      "llama_perf_context_print:       total time =  207732.85 ms /  1618 tokens\n",
      "Llama.generate: 11 prefix-match hit, remaining 1272 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final Summary:\n",
      "\n",
      "The team is designing a remote control catering to a younger demographic, featuring a rubber case\n",
      "and an LCD screen. The remote will have round buttons, including a central menu key, power button,\n",
      "and buttons for teletext, mute, and favorite channels. The number keys have been moved to an easily\n",
      "accessible location for zapping functions. The design will also include a round scroll wheel for\n",
      "volume control, but its necessity is under debate. The remote will have lights for remote finding\n",
      "and signal indication. The microphone and speech recognition component will be integrated with the\n",
      "transmitter, possibly on the side or top. The team is considering a double microphone and curving\n",
      "the remote for better handling. The remote will be technologically innovative, avoiding speech\n",
      "recognition, and will have a simple channel selection. The team is still undecided on the battery\n",
      "type, and the design is undergoing further discussion to reach a final decision. The design will be\n",
      "unique, trendy, and technologically advanced, with potential future customizable covers. However,\n",
      "there are concerns about cost issues and the availability of advanced materials.\n",
      "[DONE] Summary saved to: ./amicorpus/TS3012c\\TS3012c.summary.txt\n",
      "[INFO] Processing: ./amicorpus/TS3012d\\TS3012d.Mix-Headset.txt\n",
      "🔄 Chunking transcript...\n",
      "✅ 7 chunks created.\n",
      "📝 Summarizing chunk 1/7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  153270.50 ms /  1272 tokens (  120.50 ms per token,     8.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =   22407.17 ms /   128 runs   (  175.06 ms per token,     5.71 tokens per second)\n",
      "llama_perf_context_print:       total time =  175730.72 ms /  1400 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 2/7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  144936.92 ms /  1206 tokens (  120.18 ms per token,     8.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =   23918.12 ms /   136 runs   (  175.87 ms per token,     5.69 tokens per second)\n",
      "llama_perf_context_print:       total time =  168912.54 ms /  1342 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 3/7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145122.24 ms /  1206 tokens (  120.33 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   27931.90 ms /   160 runs   (  174.57 ms per token,     5.73 tokens per second)\n",
      "llama_perf_context_print:       total time =  173125.79 ms /  1366 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 4/7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145390.96 ms /  1206 tokens (  120.56 ms per token,     8.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26949.39 ms /   155 runs   (  173.87 ms per token,     5.75 tokens per second)\n",
      "llama_perf_context_print:       total time =  172409.62 ms /  1361 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1206 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 5/7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  145295.79 ms /  1206 tokens (  120.48 ms per token,     8.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =   18307.20 ms /   106 runs   (  172.71 ms per token,     5.79 tokens per second)\n",
      "llama_perf_context_print:       total time =  163643.80 ms /  1312 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1018 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 6/7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  122109.78 ms /  1018 tokens (  119.95 ms per token,     8.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21251.20 ms /   124 runs   (  171.38 ms per token,     5.83 tokens per second)\n",
      "llama_perf_context_print:       total time =  143411.38 ms /  1142 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 18 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Summarizing chunk 7/7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =    2207.29 ms /    18 tokens (  122.63 ms per token,     8.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6748.15 ms /    42 runs   (  160.67 ms per token,     6.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    8968.39 ms /    60 tokens\n",
      "Llama.generate: 11 prefix-match hit, remaining 937 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Generating final summary prompt...\n",
      "📚 Running final summary inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  148899.90 ms\n",
      "llama_perf_context_print: prompt eval time =  111988.05 ms /   937 tokens (  119.52 ms per token,     8.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =   51503.36 ms /   301 runs   (  171.11 ms per token,     5.84 tokens per second)\n",
      "llama_perf_context_print:       total time =  163682.43 ms /  1238 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final Summary:\n",
      "\n",
      "In the meeting, the design team discussed the product's design, focusing on the remote control. The\n",
      "remote was agreed upon to be made of rubber, with a yellow back and black components. The menu would\n",
      "be a darker yellow to improve contrast. The LCD display and speed recognition were included, but due\n",
      "to budget constraints, speech recognition was proposed to be removed. To meet the production cost of\n",
      "12.5 euros, additional changes included scrapping the special yellow color and possibly simplifying\n",
      "the design by removing curves. The remote's cover material may be changed to plastic, and the color\n",
      "option could be offered for an additional cost. The team also considered offering separate covers\n",
      "for customers who prefer custom designs.  The unique curve design was maintained, but sacrifices\n",
      "were made to reduce costs, such as potentially removing the spongy feel and simplifying the design.\n",
      "The LCD screen was kept, despite the added cost, for its added functionality. The group acknowledged\n",
      "the need for financial information during the design process to align with reality and expressed a\n",
      "need for improved communication within the team. The product was evaluated positively overall, but a\n",
      "product name was not decided upon. The team suggested that if speech recognition was a priority,\n",
      "users should be prepared to pay more for it. They also discussed improvements for the smart board's\n",
      "response time and digital pen's page limitation. The meeting concluded with a discussion about the\n",
      "project's progress and the need for a detailed report.\n",
      "[DONE] Summary saved to: ./amicorpus/TS3012d\\TS3012d.summary.txt\n"
     ]
    }
   ],
   "source": [
    "dataset_root = \"./amicorpus/\"  \n",
    "for folder in os.listdir(dataset_root):\n",
    "    folder_path = os.path.join(dataset_root, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        process_meeting_folder(folder_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
