{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import whisper\n",
    "import torchaudio\n",
    "import librosa\n",
    "import os\n",
    "import re\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "\n",
    "# Check if GPU is available \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Limit to 90% of total GPU memory\n",
    "torch.cuda.set_per_process_memory_fraction(0.9, device=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00 MB allocated, 0.00 MB reserved\n"
     ]
    }
   ],
   "source": [
    "allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "reserved = torch.cuda.memory_reserved() / 1024**2\n",
    "print(f\"{allocated:.2f} MB allocated, {reserved:.2f} MB reserved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Whisper(\n",
       "  (encoder): AudioEncoder(\n",
       "    (conv1): Conv1d(80, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (blocks): ModuleList(\n",
       "      (0-23): 24 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TextDecoder(\n",
       "    (token_embedding): Embedding(51865, 1024)\n",
       "    (blocks): ModuleList(\n",
       "      (0-23): 24 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (cross_attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_model_dir = \"./models/whisper\"\n",
    "\n",
    "\n",
    "# Load processor and model\n",
    "ASR_model = whisper.load_model('medium', download_root=custom_model_dir)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ASR_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_path, chunk_duration_sec=30):\n",
    "    \"\"\"\n",
    "    Transcribe long audio using OpenAI Whisper with manual chunking.\n",
    "    \"\"\"\n",
    "    # Load and resample audio\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "\n",
    "    # Resample to 16000 Hz\n",
    "    if sample_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        waveform = resampler(waveform)\n",
    "        sample_rate = 16000\n",
    "\n",
    "    waveform = waveform.squeeze()  # mono\n",
    "    total_samples = waveform.shape[0]\n",
    "    chunk_size = int(sample_rate * chunk_duration_sec)\n",
    "\n",
    "    transcriptions = []\n",
    "\n",
    "    for start in range(0, total_samples, chunk_size):\n",
    "        end = min(start + chunk_size, total_samples)\n",
    "        chunk = waveform[start:end].cpu().numpy()\n",
    "\n",
    "        # Whisper expects 16-bit float PCM data\n",
    "        audio_np = chunk.astype(\"float32\")\n",
    "\n",
    "        # Transcribe each chunk\n",
    "        result = ASR_model.transcribe(audio_np, language=\"en\")\n",
    "        transcriptions.append(result[\"text\"].strip())\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return \" \".join(transcriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartEncoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartDecoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_model_dir = \"./models/meetingSum-bart\"\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(custom_model_dir)\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(custom_model_dir)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bart_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, max_chunk_tokens=1024, summary_max_length=250):\n",
    "    \"\"\"Summarize each chunks with the BART model.\"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer.encode(current_chunk + sentence, truncation=False)\n",
    "        if len(tokens) <= max_chunk_tokens:\n",
    "            current_chunk += \" \" + sentence\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    all_summaries = []\n",
    "    for chunk in chunks:\n",
    "        inputs = tokenizer(\n",
    "            chunk,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=1024,  # Max input for BART-large\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            summary_ids = bart_model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_length=summary_max_length,     # 🔼 Increase this as needed (up to ~512)\n",
    "                min_length=80,                     # Optional: force a minimum length\n",
    "                length_penalty=2.0,                # Encourage longer summaries\n",
    "                num_beams=4,\n",
    "                no_repeat_ngram_size=3,\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        all_summaries.append(summary.strip())\n",
    "\n",
    "        # Free memory\n",
    "        del inputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return all_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_chunks(chunks, summary_max_length=250):\n",
    "    \"\"\"Take a list of chunk summaries and generate a final summary.\"\"\"\n",
    "    # Join all chunk summaries into one text\n",
    "    combined_text = \" \".join(chunks)\n",
    "\n",
    "    # Tokenize the combined summaries\n",
    "    inputs = tokenizer(\n",
    "        combined_text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=1024,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    ).to(device)\n",
    "\n",
    "    # Generate the final summary\n",
    "    with torch.no_grad():\n",
    "        summary_ids = bart_model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=summary_max_length,\n",
    "            min_length=80,\n",
    "            length_penalty=2.0,\n",
    "            num_beams=4,\n",
    "            no_repeat_ngram_size=3,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    final_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return final_summary.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: Okay, well I think we're ready to begin. Right, my name's Adam Duggard, we're here because of Real Reaction. We have in the group... Ebenezer Ademusoy, would you like me to spell it out? N-E-Z-E-R Your role is? I'm the marketing expert. Next we have Tarik Rammer, T-A-R-I-K, and your role in this is industrial designer. And lastly we have Dave Cochran, and you're going to be the user interface designer. Right, this is the agenda for today's meeting. As you can see, opening, acquaintance, tool training, project plan, discussion and closing. We already got through opening and partially through acquaintance. So the reason we're here, we're going to design a new remote control as you probably all know. The very broad overview is original, trendy and user friendly. Of course, we'll have to go into a bit more detail than that. Personally I think the original is going to be a very key aspect of this design. There's a lot of remote controls out there anyway, so we're going to need something that's really going to set it apart. This is how today seems to be going to work. We now have the 3K phases, as you've probably already been told, the functional conceptual and the detailed design. First one's going to be covered. user requirements spec, technical functions, working design. Second seems to be conceptual components properties, materials and the last one is a detailed analysis of our design so far. Of course you've all got the similar emails I believe. What can I say? Every user, do you want to draw your favourite animal? Sure. Whiteboard. Okay. Okay. I'm gonna make this quick since we don't have much time.  Okay, so this is not the best picture in the world. Here we have an elephant. First point, it begins with an E, same like a Beniza. Also, elephants have a very good memory, much like myself. And I can't remember back when I used to live in Nigeria, but I think I used to have a pet elephant. So elephants are big, strong and gentle, and they have great memories. letter E just like Ebeneez. Brilliant done. Thank you. Derek would you like to have a shot at a bit of artistry? It's supposed to come all the way is it? Oh you can clip them to your belt. Oh thank you. You should also have your lapel mic on as well. Does it? Oh that's good we can clip them on. Okay yeah there's this microphone as well. just somewhere just somewhere the it's just across there that's it yep looks it's it's it's it's it's it's it's Is this what we clip as well? I think so. Yeah, it'll follow you if you... Yeah. There we go. You can probably just stick it in your pocket for now. I wouldn't worry too much. You should have good dreams. Yeah, I'm destroying your outfit here.   Alright, here we have a tiger. I've always loved tigers. They're just, they're big, they're biggest cats. I did a project on cats of the wild when I was a kid and it was my favourite cat just because it looks the best. The stripes, orange, my dad used to talk about, he's from Bangladesh so he used to tell me all about them when I was a kid. And they're just the most feared of a... animals in the wild so that's why I like them. I don't know if they met me really but... Excellent, thank you very much. Dave, would you like to have a dash?    Mao porta This is somewhat a bleak reference in fact to my... I have a three year old daughter who is affectionately known as Miss Monkey. Monkeys have attitude, which I think is a good thing. And from the point of view of the study of human evolution, they and other privates are terribly interesting. So I like monkeys. This one seems to have perhaps more attitude than most.  hardly what I'd call the best drawing in the world but it'll do for now awesome not quite as feared as your average tiger but cats are one of my favorite animals. They're very independent, they're snotty as hell at the best of times and what can you say, you've got all of those qualities in an animal. Right, I think we've all managed to master every whiteboard there by the looks of it. So, on to it, Project Finance. As you can see, it's 12.5 euros per unit. Not a terrible lot as far as I'm aware. We're hoping to sell them for 25. If we're aiming for 50 million euros we're going to have to be selling an awful lot of them. Oh that was profit aim. That was a mount so that's the mount made. Yeah. Well 50 million and if you're making 12.5 euros in each one then an awful lot need to be sold. Thanks for watching. Okay, now we better actually just get on with the meat of the project. So I'm going to guess that we've all used remote controls. Any ideas of where you think a new remote control could go into this market? Well, one thing I'm aware of is that there are sort of at the very high price end of the market, there are emerging market touch screen LCD remotes that can be... programmed in much more sophisticated ways, sort of convention and models, so you get the sort of, you get, you can redesign the interface to your own needs, you can program in macros and you get a much greater degree, I mean you get these sort of 3 in 1, 5 in 1, whatever, but you can get integration between the different things that it's designed to control to a much greater extent. to turn the TV to the right channel, rewind the tape in the VCR and get it to play once it's rewound for instance. It occurs to me there might be a niche for a remote that aimed towards some of that sort of functionality but using a conventional push button design and therefore putting it into... much lower price bracket. Okay, that's true. With the price range we're looking at, going for a touch screen would probably be possibly a little bit better. But you think again, something to control multiple units in a simple fashion. Yeah, I mean, I see you get ones that you can switch between multiple units, but something that could operate between multiple units in a more integrated fashion. So there's an idea of something into which we could have some... at least limited facility for running macros. Would you be idea something along the lines of one on button would turn on say the video recorder, the TV, maybe the sound system as well all in one go? For instance, I say, or you press say the play button for the DVD player and it turns the TV on and on to the right channel as well. Um. Okay, that sounds like a good strong idea. Any takes in this? Well, I've noticed that gaming is becoming quite popular with television. When I was younger, we used to play games using a cable, using the cable subscriber cable providers. But our remote controllers get worn out really easily, and the remote controller is not a great kind of... keypad for playing games. So perhaps one that was more specialized for game playing or interactive television. They recently brought out this new remote control for people to set their favorite channels to record things. Instead of people entering again what time things start, you simply slide a bar to say what time it begins and slide another bar to say what time it ends. You have heard of Cinebarco design before. Yeah, it's taken out the... You don't have to be really clever to use a remote control. I think for gaming, you want some big buttons for up, down, left and right, shoot. You want to be able to change angles in interactive television, so you need buttons to change the camera angles and stuff like that. Okay, well we're beginning to run out of time now, so we've got a couple of ideas. We'll have to work fast. As you can see we've got 30 minutes until the next meeting so we'll have to try and decide on some of the basic functionality, how the user interface might work, that will be a key aspect especially if the idea of some kind of macro facility because you have to program it, you have to have a lot of response back or at least some kind. And we're going to maybe try and have to figure out... out what the user wants what the user wants yes okay right has anybody got anything they'd like to add at this stage anything they think that might have been missed so far when you're talking about game and stuff do you think they should have some sort of stick on it rather than oh okay that's it like control that's enough games for or is that bit ridiculous I don't see why not almost everybody's probably used to a console by now and all of them incorporate small D-pads in them. In fact, even the mobile phones these days are beginning to use them as well. So it's probably an interface that most people are used to. And that could allow easy navigation, use as a joystick as well. I think maybe the other key feature that... would be a good idea of building to it is to make something fairly sort of ergonomic something fits as comfortably as possible into the hand of course it also allows for the possibility of a more sort of slightly unconventional attractive shape for it overall so small stylish and something slightly sort of biomorphic in form which would need to be conformed to the shape of the hand more efficiently anyway okay that's definitely something that we should be able to do quite easily I would have hoped so anyway right I'd say we finish this one up, we'll get started, I'll write up what we've kind of quickly done and I'll get that out to everybody Yeah. Okay. As far as I'm aware, we leave the microphones here. Let's get told otherwise and just take the laptop for this.\n"
     ]
    }
   ],
   "source": [
    "# Sample usage\n",
    "audio_path = \"amicorpus/ES2003a/ES2003a.Mix-Headset.wav\"  # Replace with your actual audio file\n",
    "transcription = transcribe_audio(audio_path)\n",
    "print(\"Transcription:\", transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Summary: In the meeting, Adam Duggard, the group responsible for designing a new, trendy, and user-friendly remote control for an interactive television, was briefed on the design process, including functional, conceptual, and detailed design. The remote control will have an original design, a working design, and a detailed analysis of components properties, materials, and features. The team is aiming for a profit of 25 million euros, with a target selling price of 50 million euros and a goal of selling 50 million units in the next financial year. The design process will progress through functional, detailed, and conceptual phases. Tarik Rammer, T-A-R-I-K, and Dave Cochran are the industrial designer. The group agreed on ergonomics, aesthetics, and potential inclusion of speech recognition technology. The\n"
     ]
    }
   ],
   "source": [
    "summary = summarize_text(transcription)\n",
    "final_summary = summarize_chunks(summary)\n",
    "print(\"Final Summary:\", final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_txt(transcription, summary, output_path=\"output.txt\"):\n",
    "    \"\"\"Save the transcription and summary to a single text file.\"\"\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"=== TRANSCRIPTION WITH WHISPER MEDIUM===\\n\")\n",
    "        f.write(transcription + \"\\n\\n\")\n",
    "        f.write(\"=== SUMMARY WITH BART===\\n\")\n",
    "        f.write(\"\\n\".join(summary) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_txt(transcription, summary, output_path=\"meeting_notes.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
